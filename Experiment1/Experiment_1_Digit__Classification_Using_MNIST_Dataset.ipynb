{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJI5pZxmGCnE"
      },
      "outputs": [],
      "source": [
        "# Digit Classification Using MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ytFjJnIWYDp"
      },
      "outputs": [],
      "source": [
        "# Image Dimension\n",
        "img_dim = 28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfetdGFFWfS_"
      },
      "outputs": [],
      "source": [
        "import numpy as np                 # For Numerical Operations and Array Handling.\n",
        "import matplotlib.pyplot as plt    # For Plotting Graphs and Visualizations.\n",
        "\n",
        "from os.path import join           # For Joining File and Directory Paths.\n",
        "from scipy.ndimage import rotate   # For Rotating Images or Arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxs7aGbKWiPM"
      },
      "outputs": [],
      "source": [
        "train_image_path = \"/content/train-images.idx3-ubyte\"\n",
        "train_labels_path = \"/content/train-labels.idx1-ubyte\"\n",
        "test_image_path = \"/content/t10k-images.idx3-ubyte\"\n",
        "test_labels_path = \"/content/t10k-labels.idx1-ubyte\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 864
        },
        "id": "eCVsZL1hWkS-",
        "outputId": "684d62e0-3c1a-4679-b6ff-ade01e93b8bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Labels Shape:  (60000,)\n",
            "Images Shape:  (60000, 28, 28)\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxkAAAMsCAYAAAA4VG/hAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQT5JREFUeJzt3XmUVeWZN+z7gAiICKLglIgSHDCiqChKo+CIAxKMxiEaJW0wS6MSl3OiQjpxiuIAOHUcib6vbSugMRpjN5AYQ0DaaDcqiigqigwqgsogqfP9kU/e2OizqzxPDae4rrVYKzm/ffa+q0I9qV/tYj+lcrlcDgAAgExaNPYAAABA86JkAAAAWSkZAABAVkoGAACQlZIBAABkpWQAAABZKRkAAEBWSgYAAJCVkgEAAGSlZFSpuXPnRqlUimuvvTbbOadMmRKlUimmTJmS7ZxA02UdASphDSFFyWhAd999d5RKpZgxY0Zjj1IvRo4cGaVSaa0/bdq0aezRoNlo7utIRMTbb78dxx57bHTs2DE22mij+Na3vhWvvfZaY48FzcK6sIb8o4MPPjhKpVKceeaZjT3KOme9xh6A5ueWW26JDTfccM1/b9myZSNOA1STjz76KPbff//48MMP4yc/+Um0atUqrr/++ujfv38899xzsckmmzT2iECVGD9+fEydOrWxx1hnKRlkd8wxx8Smm27a2GMAVejmm2+O2bNnx/Tp02PPPfeMiIjDDjssdt555xg1alRcccUVjTwhUA1WrFgR5557blx44YVx2WWXNfY46yS/LtXErFq1Ki677LLYY489okOHDtGuXbvYd999Y/LkyV/6nuuvvz66du0abdu2jf79+8fMmTPXOmbWrFlxzDHHRKdOnaJNmzbRu3fveOSRRwrn+eSTT2LWrFmxePHiWn8M5XI5li5dGuVyudbvAfKp5nXkwQcfjD333HNNwYiI2HHHHePAAw+MBx54oPD9QOWqeQ35zC9/+cuoqamJ8847r9bvIS8lo4lZunRp3H777TFgwIC4+uqrY+TIkbFo0aIYOHBgPPfcc2sdP27cuBg9enT86Ec/iosvvjhmzpwZBxxwQCxYsGDNMS+88ELsvffe8dJLL8VFF10Uo0aNinbt2sWQIUNiwoQJyXmmT58ePXr0iLFjx9b6Y+jWrVt06NAh2rdvHyeddNLnZgHqX7WuIzU1NfHf//3f0bt377WyvfbaK+bMmRPLli2r3ScB+MqqdQ35zJtvvhlXXXVVXH311dG2bds6fezk49elmpiNN9445s6dG+uvv/6a14YNGxY77rhjjBkzJu64447PHf/qq6/G7NmzY6uttoqIiEMPPTT69OkTV199dVx33XURETF8+PDYeuut45lnnonWrVtHRMQZZ5wR/fr1iwsvvDCOOuqobLOfeeaZsc8++0Tr1q3jqaeeiptuuimmT58eM2bMiI022ijLdYC0al1H3n///Vi5cmVsscUWa2WfvfbOO+/EDjvsUPG1gC9XrWvIZ84999zYbbfd4vjjj892TurOnYwmpmXLlmu+qGtqauL999+P1atXR+/evePZZ59d6/ghQ4as+aKO+PtP+/r06ROPPfZYRPz9/7QnTZoUxx57bCxbtiwWL14cixcvjvfeey8GDhwYs2fPjrfffvtL5xkwYECUy+UYOXJk4ezDhw+PMWPGxHe/+904+uij44Ybboh77rknZs+eHTfffHMdPxPAV1Wt68jy5csjItZ8A/KPPntK3WfHAPWnWteQiIjJkyfHQw89FDfccEPdPmiyUzKaoHvuuSd22WWXaNOmTWyyySbRuXPn+O1vfxsffvjhWsdut912a722/fbbx9y5cyPi7z9dKJfLcemll0bnzp0/92fEiBEREbFw4cJ6+1i++93vxuabbx7/8R//UW/XANZWjevIZ7/WsHLlyrWyFStWfO4YoH5V4xqyevXqOPvss+N73/ve5/5dF43Dr0s1Mffee28MHTo0hgwZEueff3506dIlWrZsGVdeeWXMmTOnzuerqamJiIjzzjsvBg4c+IXHdO/evaKZi3z961+P999/v16vAfw/1bqOdOrUKVq3bh3z589fK/vstS233LLi6wBp1bqGjBs3Ll5++eW47bbb1hSczyxbtizmzp0bXbp0iQ022KDia1FMyWhiHnzwwejWrVuMHz8+SqXSmtc/a/r/2+zZs9d67ZVXXoltttkmIv7+j7AjIlq1ahUHHXRQ/oELlMvlmDt3buy2224Nfm1YV1XrOtKiRYvo2bPnF24SNm3atOjWrVu0b9++3q4P/F21riFvvvlmfPrpp/FP//RPa2Xjxo2LcePGxYQJE2LIkCH1NgP/j1+XamI+27juHx//Om3atC/dTGbixImf+z3G6dOnx7Rp0+Kwww6LiIguXbrEgAED4rbbbvvCnw4uWrQoOU9dHhv3Ree65ZZbYtGiRXHooYcWvh/Io5rXkWOOOSaeeeaZzxWNl19+OSZNmhTf+c53Ct8PVK5a15Djjz8+JkyYsNafiIjDDz88JkyYEH369Emeg3zcyWgEd955Z/zud79b6/Xhw4fHoEGDYvz48XHUUUfFEUccEa+//nrceuutsdNOO8VHH3201nu6d+8e/fr1i9NPPz1WrlwZN9xwQ2yyySZxwQUXrDnmpptuin79+kXPnj1j2LBh0a1bt1iwYEFMnTo15s2bF88///yXzjp9+vTYf//9Y8SIEYX/4Kpr165x3HHHRc+ePaNNmzbxpz/9Ke6///7o1atX/PCHP6z9Jwgo1FzXkTPOOCN+9atfxRFHHBHnnXdetGrVKq677rrYbLPN4txzz639JwhIao5ryI477hg77rjjF2bbbrutOxgNTMloBLfccssXvj506NAYOnRovPvuu3HbbbfFE088ETvttFPce++98e///u8xZcqUtd5z8sknR4sWLeKGG26IhQsXxl577RVjx4793CMgd9ppp5gxY0b87Gc/i7vvvjvee++96NKlS+y2225Zd8E88cQT489//nM89NBDsWLFiujatWtccMEF8dOf/tTvP0JmzXUdad++fUyZMiXOOeec+MUvfhE1NTUxYMCAuP7666Nz587ZrgPruua6htB0lMq2ZQYAADLybzIAAICslAwAACArJQMAAMhKyQAAALJSMgAAgKyUDAAAICslAwAAyKrWm/GVSqX6nAOopWre2sY6Ak1Dta4j1hBoGmqzhriTAQAAZKVkAAAAWSkZAABAVkoGAACQlZIBAABkpWQAAABZKRkAAEBWSgYAAJCVkgEAAGSlZAAAAFkpGQAAQFZKBgAAkJWSAQAAZKVkAAAAWSkZAABAVkoGAACQlZIBAABkpWQAAABZKRkAAEBWSgYAAJCVkgEAAGSlZAAAAFkpGQAAQFZKBgAAkJWSAQAAZKVkAAAAWSkZAABAVkoGAACQlZIBAABkpWQAAABZrdfYAwDQvOyxxx7J/Mwzz0zmJ598cjIfN25cMh8zZkwyf/bZZ5M5AJVzJwMAAMhKyQAAALJSMgAAgKyUDAAAICslAwAAyErJAAAAslIyAACArJQMAAAgq1K5XC7X6sBSqb5noUDLli2TeYcOHep9hqJNtDbYYINkvsMOOyTzH/3oR8n82muvTeYnnHBCMl+xYkUyv+qqq5J5RMTPfvazwmPqUy2/ZJsk60j169WrV+ExkyZNSuYbbbRRpmm+2IcffpjMN9lkk3q9fjWo1nXEGkJTcOCBBybz++67L5n3798/mb/88st1nqmh1WYNcScDAADISskAAACyUjIAAICslAwAACArJQMAAMhKyQAAALJSMgAAgKzWa+wBqsnWW2+dzNdff/1k3rdv32Ter1+/ZN6xY8dkfvTRRyfzpmDevHnJfPTo0cn8qKOOSubLli1L5s8//3wy/8Mf/pDMobnba6+9kvlDDz1UeI6iPXuKnq9e9HW8atWqZF60D8bee++dzJ999tmKrk/ztd9++xUeU/T3b8KECbnGoZHsueeeyfyZZ55poEmaNncyAACArJQMAAAgKyUDAADISskAAACyUjIAAICslAwAACArJQMAAMjKPhn/oFevXsl80qRJybzo2fDrgpqammR+ySWXJPOPPvoomd93333JfP78+cn8gw8+SOYvv/xyMoemboMNNkjmu+++ezK/9957k/kWW2xR55nqavbs2cn8l7/8ZTK///77k/nTTz+dzIvWqSuvvDKZ03wNGDCg8Jjtttsumdsno+lr0SL9M/htt902mXft2jWZl0qlOs9UjdzJAAAAslIyAACArJQMAAAgKyUDAADISskAAACyUjIAAICslAwAACAr+2T8gzfffDOZv/fee8m8qe+TMW3atMJjlixZksz333//ZL5q1apk/utf/7pwBuCru+2225L5CSec0ECTfHVFe3lsuOGGyfwPf/hDMi/a62CXXXZJ5qy7Tj755MJjpk6d2gCTUJ+K9gMaNmxYMi/ab2jWrFl1nqkauZMBAABkpWQAAABZKRkAAEBWSgYAAJCVkgEAAGSlZAAAAFkpGQAAQFb2yfgH77//fjI///zzk/mgQYOS+V//+tdkPnr06GRe5LnnnkvmBx98cOE5Pv7442T+zW9+M5kPHz688BrAV7fHHnsk8yOOOCKZl0qliq5ftAdFRMRvfvObZH7ttdcm83feeSeZF62lH3zwQTI/4IADknmlnyOarxYt/Gx2XXD77bdX9P7Zs2dnmqS6+WoBAACyUjIAAICslAwAACArJQMAAMhKyQAAALJSMgAAgKyUDAAAICv7ZNTBxIkTk/mkSZOS+bJly5L5rrvumsxPPfXUZF707PmiPTBq44UXXkjmp512WsXXgHVZr169kvmTTz6ZzDfaaKNkXi6Xk/njjz+ezE844YRkHhHRv3//ZH7JJZck86Jn1C9atCiZP//888m8pqYmmRftNbL77rsn82effTaZ03TtsssuyXyzzTZroEloTB06dKjo/UXr9LrCnQwAACArJQMAAMhKyQAAALJSMgAAgKyUDAAAICslAwAAyErJAAAAsrJPRkZLly6t6P0ffvhhRe8fNmxYMv+3f/u3wnMUPT8eqMz222+fzM8///xkXvT89sWLFyfz+fPnJ/N77rknmX/00UfJPCLit7/9bUV5Y2vbtm0yP/fcc5P5iSeemHMcGtDhhx+ezIv+blAdivY72XbbbSs6/9tvv13R+5sLdzIAAICslAwAACArJQMAAMhKyQAAALJSMgAAgKyUDAAAICslAwAAyMo+GU3IyJEjk/kee+yRzPv375/MDzrooMIZfv/73xceA3y51q1bJ/Nrr702mRc9p3/ZsmXJ/OSTT07mM2bMSOb2ASi29dZbN/YI1JMddtih4nO88MILGSahPhWtw0X7aLzyyivJvGidXle4kwEAAGSlZAAAAFkpGQAAQFZKBgAAkJWSAQAAZKVkAAAAWSkZAABAVvbJaEI+/vjjZD5s2LBk/uyzzybzX/3qV4UzTJ48OZkXPWP/pptuSublcrlwBqhmu+22WzIv2gejyLe+9a1k/oc//KGi8wOVeeaZZxp7hKq30UYbJfNDDz00mZ900knJ/JBDDqnzTP/o5z//eTJfsmRJRedvLtzJAAAAslIyAACArJQMAAAgKyUDAADISskAAACyUjIAAICslAwAACAr+2RUkTlz5iTzoUOHJvO77rqr8Brf+973KsrbtWuXzMeNG5fM58+fn8yhqbvuuuuSealUSuZF+1zYB6NyLVqkf75WU1PTQJPQHHXq1KlRr7/rrrsm86I16KCDDkrmX/va15L5+uuvn8xPPPHEZB5R/DW6fPnyZD5t2rRkvnLlymS+3nrpb4//67/+K5nzd+5kAAAAWSkZAABAVkoGAACQlZIBAABkpWQAAABZKRkAAEBWSgYAAJCVkgEAAGRlM75mZMKECcl89uzZheco2kjswAMPTOZXXHFFMu/atWsyv/zyy5P522+/ncyhvg0aNCiZ9+rVK5mXy+Vk/sgjj9R1JOqoaLO9ov+NnnvuuYzT0JQUbfJW9HcjIuLWW29N5j/5yU/qNFNd7bLLLsm8aDO+1atXJ/NPPvkkmb/44ovJ/M4770zmEREzZsxI5kWbki5YsCCZz5s3L5m3bds2mc+aNSuZ83fuZAAAAFkpGQAAQFZKBgAAkJWSAQAAZKVkAAAAWSkZAABAVkoGAACQlX0y1iEzZ84sPObYY49N5kceeWQyv+uuu5L5D3/4w2S+3XbbJfODDz44mUN9K3p++vrrr5/MFy5cmMz/7d/+rc4zrWtat26dzEeOHFnR+SdNmpTML7744orOT9N1xhlnJPM33nij8Bx9+/bNNc5X8uabbybziRMnJvOXXnopmf/lL3+p60gN7rTTTkvmnTt3TuavvfZaznHWWe5kAAAAWSkZAABAVkoGAACQlZIBAABkpWQAAABZKRkAAEBWSgYAAJCVfTL4nCVLliTzX//618n89ttvT+brrZf+K7fffvsl8wEDBiTzKVOmJHNobCtXrkzm8+fPb6BJmq6ifTAuueSSZH7++ecn83nz5iXzUaNGJfOPPvoomdN8XX311Y09ArVw4IEHVvT+hx56KNMk6zZ3MgAAgKyUDAAAICslAwAAyErJAAAAslIyAACArJQMAAAgKyUDAADIyj4Z65Bddtml8Jhjjjkmme+5557JvGgfjCIvvvhiMv/jH/9Y0fmhsT3yyCONPUKj69WrVzIv2ufiuOOOS+YPP/xwMj/66KOTObBumzBhQmOP0Cy4kwEAAGSlZAAAAFkpGQAAQFZKBgAAkJWSAQAAZKVkAAAAWSkZAABAVvbJqCI77LBDMj/zzDOT+be//e3Ca2y++eZ1mqmu/va3vyXz+fPnJ/Oampqc40CdlUqlivIhQ4Yk8+HDh9d1pCbnnHPOSeaXXnppMu/QoUMyv++++5L5ySefnMwBqH/uZAAAAFkpGQAAQFZKBgAAkJWSAQAAZKVkAAAAWSkZAABAVkoGAACQlX0yGlDRHhQnnHBCMi/aB2Obbbap60jZzZgxI5lffvnlyfyRRx7JOQ5kVy6XK8qL1oHRo0cn8zvvvDOZv/fee8l87733Tubf+973kvmuu+6azCMivva1ryXzN998M5k/8cQTyfzmm28unAHgyxTtZ7T99tsn87/85S85x2m23MkAAACyUjIAAICslAwAACArJQMAAMhKyQAAALJSMgAAgKyUDAAAICv7ZNTBZpttlsx32mmnZD527NhkvuOOO9Z5ptymTZuWzK+55ppk/vDDDyfzmpqaOs8EzUnLli2T+RlnnJHMjz766GS+dOnSZL7ddtsl8xz+/Oc/J/PJkycn88suuyznOACfU7SfUYsWfgafg88iAACQlZIBAABkpWQAAABZKRkAAEBWSgYAAJCVkgEAAGSlZAAAAFmtU/tkdOrUKZnfdtttybxXr17JvFu3bnUdKauiZ9OPGjWq8BxPPPFEMl++fHmdZoLmZurUqcn8mWeeSeZ77rlnRdfffPPNk3nRfj5F3nvvvWR+//33F55j+PDhFc0A0Jj22WefZH733Xc3zCBVzp0MAAAgKyUDAADISskAAACyUjIAAICslAwAACArJQMAAMhKyQAAALKqmn0y+vTpU3jM+eefn8z32muvZL7VVlvVaabcPvnkk2Q+evToZH7FFVck848//rjOMwGfN2/evGT+7W9/O5n/8Ic/TOaXXHJJnWeqixtvvDGZ33LLLcn81VdfzTkOQIMrlUqNPcI6wZ0MAAAgKyUDAADISskAAACyUjIAAICslAwAACArJQMAAMhKyQAAALJSMgAAgKyqZjO+o446KssxlXjxxReT+aOPPprMV69encxHjRqVzJcsWZLMgcY3f/78ZD5y5MiKcgDSHn/88WT+ne98p4EmWbe5kwEAAGSlZAAAAFkpGQAAQFZKBgAAkJWSAQAAZKVkAAAAWSkZAABAVqVyuVyu1YGlUn3PAtRCLb9kmyTrCDQN1bqOWEOgaajNGuJOBgAAkJWSAQAAZKVkAAAAWSkZAABAVkoGAACQlZIBAABkpWQAAABZKRkAAEBWSgYAAJCVkgEAAGSlZAAAAFkpGQAAQFZKBgAAkJWSAQAAZKVkAAAAWSkZAABAVkoGAACQlZIBAABkpWQAAABZKRkAAEBWSgYAAJCVkgEAAGSlZAAAAFmVyuVyubGHAAAAmg93MgAAgKyUDAAAICslAwAAyErJAAAAslIyAACArJQMAAAgKyUDAADISskAAACyUjIAAICslAwAACArJQMAAMhKyQAAALJSMgAAgKyUDAAAICslAwAAyErJAAAAslIyAACArJQMAAAgKyUDAADISskAAACyUjIAAICslAwAACArJQMAAMhKyahSc+fOjVKpFNdee222c06ZMiVKpVJMmTIl2zmBpss6AlTCGkKKktGA7r777iiVSjFjxozGHqVevPzyy3HOOedE3759o02bNlEqlWLu3LmNPRY0K819HYmIuP/++2P33XePNm3aROfOnePUU0+NxYsXN/ZY0Cw09zVk/Pjxcdxxx0W3bt1igw02iB122CHOPffcWLJkSWOPts5RMshm6tSpMXr06Fi2bFn06NGjsccBqtAtt9wSJ5xwQnTq1Cmuu+66GDZsWNx///1x4IEHxooVKxp7PKCJO+200+Kll16Kk046KUaPHh2HHnpojB07NvbZZ59Yvnx5Y4+3TlmvsQeg+Rg8eHAsWbIk2rdvH9dee20899xzjT0SUEVWrVoVP/nJT2K//faLJ598MkqlUkRE9O3bN4488sj41a9+FWeddVYjTwk0ZQ8++GAMGDDgc6/tscceccopp8R9990XP/jBDxpnsHWQOxlNzKpVq+Kyyy6LPfbYIzp06BDt2rWLfffdNyZPnvyl77n++uuja9eu0bZt2+jfv3/MnDlzrWNmzZoVxxxzTHTq1CnatGkTvXv3jkceeaRwnk8++SRmzZpVq19V6NSpU7Rv377wOKB+Ves6MnPmzFiyZEkcd9xxawpGRMSgQYNiww03jPvvv7/wWkDlqnUNiYi1CkZExFFHHRURES+99FLh+8lHyWhili5dGrfffnsMGDAgrr766hg5cmQsWrQoBg4c+IV3BsaNGxejR4+OH/3oR3HxxRfHzJkz44ADDogFCxasOeaFF16IvffeO1566aW46KKLYtSoUdGuXbsYMmRITJgwITnP9OnTo0ePHjF27NjcHypQT6p1HVm5cmVERLRt23atrG3btvHXv/41ampqavEZACpRrWvIl3n33XcjImLTTTf9Su/nKyrTYO66665yRJSfeeaZLz1m9erV5ZUrV37utQ8++KC82Wablf/5n/95zWuvv/56OSLKbdu2Lc+bN2/N69OmTStHRPmcc85Z89qBBx5Y7tmzZ3nFihVrXqupqSn37du3vN122615bfLkyeWIKE+ePHmt10aMGFGnj/Waa64pR0T59ddfr9P7gLTmvI4sWrSoXCqVyqeeeurnXp81a1Y5IsoRUV68eHHyHEBac15Dvsypp55abtmyZfmVV175Su/nq3Eno4lp2bJlrL/++hERUVNTE++//36sXr06evfuHc8+++xaxw8ZMiS22mqrNf99r732ij59+sRjjz0WERHvv/9+TJo0KY499thYtmxZLF68OBYvXhzvvfdeDBw4MGbPnh1vv/32l84zYMCAKJfLMXLkyLwfKFBvqnUd2XTTTePYY4+Ne+65J0aNGhWvvfZaPPXUU3HcccdFq1atIiL8w01oANW6hnyR//N//k/ccccdce6558Z2221X5/fz1SkZTdA999wTu+yyS7Rp0yY22WST6Ny5c/z2t7+NDz/8cK1jv+gLZvvtt1/z6NhXX301yuVyXHrppdG5c+fP/RkxYkRERCxcuLBePx6g4VXrOnLbbbfF4YcfHuedd1584xvfiP322y969uwZRx55ZEREbLjhhlmuA6RV6xryj5566qk49dRTY+DAgXH55ZdnPz9pni7VxNx7770xdOjQGDJkSJx//vnRpUuXaNmyZVx55ZUxZ86cOp/vs99fPu+882LgwIFfeEz37t0rmhloWqp5HenQoUM8/PDD8eabb8bcuXOja9eu0bVr1+jbt2907tw5OnbsmOU6wJer5jXkM88//3wMHjw4dt5553jwwQdjvfV8y9vQfMabmAcffDC6desW48eP/9zTVT5r+v/b7Nmz13rtlVdeiW222SYiIrp16xYREa1atYqDDjoo/8BAk9Mc1pGtt946tt5664iIWLJkSfzXf/1XHH300Q1ybVjXVfsaMmfOnDj00EOjS5cu8dhjj7kD2kj8ulQT07Jly4iIKJfLa16bNm1aTJ069QuPnzhx4ud+j3H69Okxbdq0OOywwyIiokuXLjFgwIC47bbbYv78+Wu9f9GiRcl56vLYOKBpaG7ryMUXXxyrV6+Oc8455yu9H6ibal5D3n333TjkkEOiRYsW8cQTT0Tnzp0L30P9cCejEdx5553xu9/9bq3Xhw8fHoMGDYrx48fHUUcdFUcccUS8/vrrceutt8ZOO+0UH3300Vrv6d69e/Tr1y9OP/30WLlyZdxwww2xySabxAUXXLDmmJtuuin69esXPXv2jGHDhkW3bt1iwYIFMXXq1Jg3b148//zzXzrr9OnTY//9948RI0YU/oOrDz/8MMaMGRMREU8//XRERIwdOzY6duwYHTt2jDPPPLM2nx6gFprrOnLVVVfFzJkzo0+fPrHeeuvFxIkT4/e//3384he/iD333LP2nyAgqbmuIYceemi89tprccEFF8Sf/vSn+NOf/rQm22yzzeLggw+uxWeHLBrtuVbroM8eG/dlf956661yTU1N+Yorrih37dq13Lp16/Juu+1WfvTRR8unnHJKuWvXrmvO9dlj46655pryqFGjyl//+tfLrVu3Lu+7777l559/fq1rz5kzp3zyySeXN99883KrVq3KW221VXnQoEHlBx98cM0xlT427rOZvujPP84OfHXNfR159NFHy3vttVe5ffv25Q022KC89957lx944IFKPmXAP2jua0jqY+vfv38FnznqqlQu/8O9MAAAgAr5NxkAAEBWSgYAAJCVkgEAAGSlZAAAAFkpGQAAQFZKBgAAkJWSAQAAZFXrHb9LpVJ9zgHUUjVvbWMdgaahWtcRawg0DbVZQ9zJAAAAslIyAACArJQMAAAgKyUDAADISskAAACyUjIAAICslAwAACArJQMAAMhKyQAAALJSMgAAgKyUDAAAICslAwAAyErJAAAAslIyAACArJQMAAAgKyUDAADISskAAACyUjIAAICslAwAACArJQMAAMhKyQAAALJSMgAAgKyUDAAAICslAwAAyErJAAAAslIyAACArJQMAAAgKyUDAADISskAAACyUjIAAICslAwAACArJQMAAMhKyQAAALJSMgAAgKyUDAAAICslAwAAyErJAAAAslIyAACArJQMAAAgq/UaewDWLZdcckky/9nPfpbMW7RI9+IBAwYk8z/84Q/JHABoXO3bt0/mG264YTI/4ogjknnnzp2T+XXXXZfMV65cmcz5O3cyAACArJQMAAAgKyUDAADISskAAACyUjIAAICslAwAACArJQMAAMjKPhlkNXTo0GR+4YUXJvOampqKrl8ulyt6PwDw1W2zzTbJvOj7gIiIffbZJ5nvvPPOdRmpzrbYYotkfvbZZ9fr9ZsLdzIAAICslAwAACArJQMAAMhKyQAAALJSMgAAgKyUDAAAICslAwAAyMo+GWTVtWvXZN6mTZsGmgT4In369EnmJ510UjLv379/4TW++c1v1mmm/+28885L5u+8804y79evXzK/9957k/m0adOSOTRnO+64YzL/8Y9/nMxPPPHEZN62bdvCGUqlUjJ/6623kvmyZcuSeY8ePZL5sccem8xvvvnmZD5r1qxkvq5wJwMAAMhKyQAAALJSMgAAgKyUDAAAICslAwAAyErJAAAAslIyAACArOyTQZ0cdNBByfyss86q6PxFz5YeNGhQMl+wYEFF14dqd9xxxyXzG2+8MZlvuummybzo+fUREVOmTEnmnTt3TubXXHNN4TVSimYsuv7xxx9f0fWhMXXo0CGZX3311cm8aA1p3759nWeqq9mzZyfzgQMHJvNWrVol86LvNYrWwaKcv3MnAwAAyErJAAAAslIyAACArJQMAAAgKyUDAADISskAAACyUjIAAICs7JPB5/Tr1y+Z33XXXcm86PncRYqej//GG29UdH5o6tZbL70s9+7dO5n/6le/SuYbbLBBMv/jH/+YzH/+858n84iIP/3pT8m8devWyfyBBx5I5occckjhDCkzZsyo6P3QlB111FHJ/Ac/+EEDTfLF5syZU3jMwQcfnMzfeuutZN69e/c6zUT9cCcDAADISskAAACyUjIAAICslAwAACArJQMAAMhKyQAAALJSMgAAgKzsk8HnnHLKKcl8yy23rOj8U6ZMSebjxo2r6PxQ7U466aRkfvvtt1d0/ieffDKZH3fcccl86dKlFV2/NteodB+MefPmJfN77rmnovNDU/ad73ynXs8/d+7cZP7MM88k8wsvvLDwGkX7YBTp0aNHRe8nD3cyAACArJQMAAAgKyUDAADISskAAACyUjIAAICslAwAACArJQMAAMjKPhnrkE033bTwmH/+539O5jU1Ncl8yZIlyfwXv/hF4QzQnP385z9P5j/5yU+SeblcTuY333xzMr/kkkuSeY59MIr89Kc/rdfzn3322cl80aJF9Xp9aEzDhg1L5qeddloy//3vf5/MX3311WS+cOHCZN4QNttss8YegXAnAwAAyEzJAAAAslIyAACArJQMAAAgKyUDAADISskAAACyUjIAAICs7JPRjGyzzTbJ/KGHHqr3GcaMGZPMJ0+eXO8zQGO67LLLknnRPhirVq1K5k888UQyv/DCC5P58uXLk3mRNm3aFB5zyCGHJPOtt946mZdKpWRetN/Oww8/nMyhOXvnnXeS+ciRIxtmkEa0zz77NPYIhDsZAABAZkoGAACQlZIBAABkpWQAAABZKRkAAEBWSgYAAJCVkgEAAGRln4xm5NBDD03mu+yyS8XX+M///M9kfuONN1Z8DWjKOnbsmMzPOOOMZF4ul5N50T4YQ4YMSeaV6t69ezK/7777Cs+xxx57VDTDgw8+mMx/+ctfVnR+oP6cffbZybxdu3b1PkPPnj0rev+f//znZD516tSKzr+ucCcDAADISskAAACyUjIAAICslAwAACArJQMAAMhKyQAAALJSMgAAgKxK5aKHtn92YKlU37NQoOj5+HfffXcyr82zqYueDX3ssccm8wULFhReg8rU8ku2SWoO60iXLl2S+TvvvFPR+bt165bMV6xYkcy///3vJ/PBgwcn85133jmZb7jhhsk8ovjvaFH+7W9/O5n/5je/KZyBtGpdR5rDGtLYNthgg2S+0047JfMRI0Yk88MPP7zOM/1vLVqkfwZeU1NT0fmL1ukBAwYk8zlz5lR0/eagNmuIOxkAAEBWSgYAAJCVkgEAAGSlZAAAAFkpGQAAQFZKBgAAkJWSAQAAZKVkAAAAWa3X2APw/2yzzTbJ/KGHHqr3GV577bVkbrM91nWrVq1K5osWLUrmnTt3Tuavv/56Mq/vTdSKNqlaunRp4Tm22GKLZL548eJkbrM9+HKtWrVK5rvttlsyL/peoujrd/ny5cm8aA2ZOnVqMo+IOPTQQ5N50YaCRdZbL/3tb9GGoDfeeGMyL/r/iXWFOxkAAEBWSgYAAJCVkgEAAGSlZAAAAFkpGQAAQFZKBgAAkJWSAQAAZGWfjCbkwgsvTOY1NTX1PsNVV11V79eAarZkyZJkPmTIkGT+6KOPJvNOnTol8zlz5iTzhx9+OJnffffdyfz9999P5vfff38yjyh+zn5tzgHrqvXXXz+ZF+0hMX78+Iqu/7Of/SyZT5o0KZk//fTTybxojavNNXbeeefCc6QU7Vd05ZVXJvM333wzmU+cODGZr1y5Mpk3F+5kAAAAWSkZAABAVkoGAACQlZIBAABkpWQAAABZKRkAAEBWSgYAAJCVfTIaUK9evZL5IYccUq/XL3p+fkTEyy+/XK8zQHM3bdq0ZF70fPbGtt9++yXz/v37F56jaE+f1157rU4zQXPSqlWrZF60T8X5559f0fUff/zxZD5mzJhkXrRXUNEa99hjjyXziIiePXsm81WrViXzX/7yl8m8aJ+Nb33rW8n8vvvuS+b/8R//kcyvvvrqZP7BBx8k89p47rnnKj5HpdzJAAAAslIyAACArJQMAAAgKyUDAADISskAAACyUjIAAICslAwAACCrUrlcLtfqwFKpvmdp9hYuXJjMN95444rO/5e//CWZH3bYYYXn+OijjyqagfpXyy/ZJsk60vQNHDgwmdfmGfdFf0e32GKLZL5o0aLCa1CZal1HqmENadmyZTK//PLLk/l5552XzD/++ONkftFFFyXz+++/P5kX7dHQu3fvZD527NiK3h8R8eqrrybz008/PZlPnjw5mW+00UbJvG/fvsn8xBNPTOaDBw9O5u3atUvmtfHWW28l82233bbia6TUZg1xJwMAAMhKyQAAALJSMgAAgKyUDAAAICslAwAAyErJAAAAslIyAACArOyT0YD+9re/JfOampqKzn/yyScn8//7f/9vReenaajW59tHWEeag6J1LMI+GdWgWteRalhDivZwGDNmTDL/5JNPkvlpp52WzH//+98n8z59+iTz73//+8m8aM+ttm3bJvN/+Zd/SeYREXfddVcyL9ojorGdcMIJyfy73/1uxdc455xzknnRXiOVsk8GAADQ4JQMAAAgKyUDAADISskAAACyUjIAAICslAwAACArJQMAAMjKPhkZFT3XeejQocm80n0yunXrlszfeOONis5P01Ctz7ePsI5Ug4EDBybzxx57rPAc9slo+qp1HamGNWT+/PnJvHPnzsl85cqVyXzWrFnJvF27dsm8e/fuybxSI0eOTOZXXnll4Tlqsx8Pjcs+GQAAQINTMgAAgKyUDAAAICslAwAAyErJAAAAslIyAACArJQMAAAgq/Uae4Bq0qtXr2R+0EEHJfOifTBWrVqVzG+66aZkvmDBgmQOUKRovx0g7d13303mRftktG7dOpnvuuuudZ7pHxXtdfPHP/4xmU+cODGZz507N5nbA2Pd4U4GAACQlZIBAABkpWQAAABZKRkAAEBWSgYAAJCVkgEAAGSlZAAAAFnZJ6MOOnbsmMw333zzis7/9ttvJ/PzzjuvovMDFHnqqaeSeYsWxT+bKtoTCJqz/fbbL5kPGTIkme++++7JfOHChcn8zjvvTOYffPBBMi/aswtqy50MAAAgKyUDAADISskAAACyUjIAAICslAwAACArJQMAAMhKyQAAALKyTwYAa8ycOTOZz549u/Ac3bp1S+bf+MY3kvmiRYsKrwFN1bJly5L5r3/964pyqBbuZAAAAFkpGQAAQFZKBgAAkJWSAQAAZKVkAAAAWSkZAABAVkoGAACQlZIBAABkZTO+Opg1a1Yy//Of/5zM+/Xrl3McgAZ3xRVXFB5z++23J/PLL788mZ911lnJ/MUXXyycAYDG5U4GAACQlZIBAABkpWQAAABZKRkAAEBWSgYAAJCVkgEAAGSlZAAAAFmVyuVyuVYHlkr1PQtQC7X8km2SrCPVb6ONNio85oEHHkjmBx10UDIfP358Mv/+97+fzD/++ONkTvWuI9YQaBpqs4a4kwEAAGSlZAAAAFkpGQAAQFZKBgAAkJWSAQAAZKVkAAAAWSkZAABAVvbJgCpTrc+3j7COrCuK9tK4/PLLk/npp5+ezHfZZZdk/uKLLyZzqncdsYZA02CfDAAAoMEpGQAAQFZKBgAAkJWSAQAAZKVkAAAAWSkZAABAVkoGAACQlX0yoMpU6/PtI6wj0FRU6zpiDYGmwT4ZAABAg1MyAACArJQMAAAgKyUDAADISskAAACyUjIAAICslAwAACCrWu+TAQAAUBvuZAAAAFkpGQAAQFZKBgAAkJWSAQAAZKVkAAAAWSkZAABAVkoGAACQlZIBAABkpWQAAABZKRkAAEBWSgYAAJCVkgEAAGSlZAAAAFkpGQAAQFZKBgAAkJWSAQAAZKVkAAAAWSkZAABAVkoGAACQlZIBAABkpWQAAABZKRkAAEBWSgYAAJCVklGl5s6dG6VSKa699tps55wyZUqUSqWYMmVKtnMCTZd1BKiENYQUJaMB3X333VEqlWLGjBmNPUq9ePnll+Occ86Jvn37Rps2baJUKsXcuXMbeyxoVpr7OjJhwoQYOHBgbLnlltG6dev42te+Fsccc0zMnDmzsUeDZqG5ryG+F2k6lAyymTp1aowePTqWLVsWPXr0aOxxgCr0P//zP7HxxhvH8OHD4+abb47TTz89/vrXv8Zee+0Vzz//fGOPBzRxvhdpOtZr7AFoPgYPHhxLliyJ9u3bx7XXXhvPPfdcY48EVJnLLrtsrdd+8IMfxNe+9rW45ZZb4tZbb22EqYBq4XuRpsOdjCZm1apVcdlll8Uee+wRHTp0iHbt2sW+++4bkydP/tL3XH/99dG1a9do27Zt9O/f/wt/rWDWrFlxzDHHRKdOnaJNmzbRu3fveOSRRwrn+eSTT2LWrFmxePHiwmM7deoU7du3LzwOqF/VvI58kS5dusQGG2wQS5Ys+UrvB+qmmtcQ34s0HUpGE7N06dK4/fbbY8CAAXH11VfHyJEjY9GiRTFw4MAvbOPjxo2L0aNHx49+9KO4+OKLY+bMmXHAAQfEggUL1hzzwgsvxN577x0vvfRSXHTRRTFq1Kho165dDBkyJCZMmJCcZ/r06dGjR48YO3Zs7g8VqCfNYR1ZsmRJLFq0KP7nf/4nfvCDH8TSpUvjwAMPrPX7ga+uOawhND6/LtXEbLzxxjF37txYf/3117w2bNiw2HHHHWPMmDFxxx13fO74V199NWbPnh1bbbVVREQceuih0adPn7j66qvjuuuui4iI4cOHx9Zbbx3PPPNMtG7dOiIizjjjjOjXr19ceOGFcdRRRzXQRwc0hOawjuy9997x8ssvR0TEhhtuGJdcckmceuqpWa8BfLHmsIbQ+NzJaGJatmy55ou6pqYm3n///Vi9enX07t07nn322bWOHzJkyJov6oiIvfbaK/r06ROPPfZYRES8//77MWnSpDj22GNj2bJlsXjx4li8eHG89957MXDgwJg9e3a8/fbbXzrPgAEDolwux8iRI/N+oEC9aQ7ryF133RW/+93v4uabb44ePXrE8uXL429/+1ut3w98dc1hDaHxuZPRBN1zzz0xatSomDVrVnz66adrXt92223XOna77bZb67Xtt98+HnjggYj4+08XyuVyXHrppXHppZd+4fUWLlz4ucUBqH7Vvo7ss88+a/7z8ccfv+YpMTmfxw98uWpfQ2h8SkYTc++998bQoUNjyJAhcf7550eXLl2iZcuWceWVV8acOXPqfL6ampqIiDjvvPNi4MCBX3hM9+7dK5oZaFqa2zqy8cYbxwEHHBD33XefkgENoLmtITQOJaOJefDBB6Nbt24xfvz4KJVKa14fMWLEFx4/e/bstV575ZVXYptttomIiG7dukVERKtWreKggw7KPzDQ5DTHdWT58uXx4YcfNsq1YV3THNcQGp5/k9HEtGzZMiIiyuXymtemTZsWU6dO/cLjJ06c+LnfY5w+fXpMmzYtDjvssIj4+6MfBwwYELfddlvMnz9/rfcvWrQoOU+lj54EGl41ryMLFy5c67W5c+fGf/7nf0bv3r0L3w9UrprXEJoOdzIawZ133hm/+93v1np9+PDhMWjQoBg/fnwcddRRccQRR8Trr78et956a+y0007x0UcfrfWe7t27R79+/eL000+PlStXxg033BCbbLJJXHDBBWuOuemmm6Jfv37Rs2fPGDZsWHTr1i0WLFgQU6dOjXnz5iV30Z0+fXrsv//+MWLEiMJ/cPXhhx/GmDFjIiLi6aefjoiIsWPHRseOHaNjx45x5pln1ubTA9RCc11HevbsGQceeGD06tUrNt5445g9e3bccccd8emnn8ZVV11V+08QkNRc1xDfizQhZRrMXXfdVY6IL/3z1ltvlWtqaspXXHFFuWvXruXWrVuXd9ttt/Kjjz5aPuWUU8pdu3Zdc67XX3+9HBHla665pjxq1Kjy17/+9XLr1q3L++67b/n5559f69pz5swpn3zyyeXNN9+83KpVq/JWW21VHjRoUPnBBx9cc8zkyZPLEVGePHnyWq+NGDGi8OP7bKYv+vOPswNfXXNfR0aMGFHu3bt3eeONNy6vt9565S233LJ8/PHHl//7v/+7kk8b8P9r7muI70WajlK5/A/3wgAAACrk32QAAABZKRkAAEBWSgYAAJCVkgEAAGSlZAAAAFkpGQAAQFZKBgAAkFWtd/wulUr1OQdQS9W8tY11BJqGal1HrCHQNNRmDXEnAwAAyErJAAAAslIyAACArJQMAAAgKyUDAADISskAAACyUjIAAICslAwAACArJQMAAMhKyQAAALJSMgAAgKyUDAAAICslAwAAyErJAAAAslIyAACArJQMAAAgKyUDAADISskAAACyUjIAAICslAwAACArJQMAAMhKyQAAALJSMgAAgKyUDAAAICslAwAAyErJAAAAslIyAACArJQMAAAgKyUDAADISskAAACyWq+xB6D2brzxxmR+9tlnJ/OZM2cWXmPQoEHJ/I033ig8BwAA6zZ3MgAAgKyUDAAAICslAwAAyErJAAAAslIyAACArJQMAAAgKyUDAADISskAAACyshlfE7LNNtsk85NOOimZ19TUJPMePXoUzrDjjjsmc5vxQdO2/fbbJ/NWrVol8/322y+Z33zzzYUzFK1Fje3hhx9O5scff3wyX7VqVc5xoKoUrSF9+/ZN5ldccUXhNf7pn/6pTjPRNLmTAQAAZKVkAAAAWSkZAABAVkoGAACQlZIBAABkpWQAAABZKRkAAEBW9sloQhYtWpTM//jHPybzwYMH5xwHaATf/OY3k/nQoUOT+Xe+851k3qJF+mdLW265ZTKvzR4Y5XK58JjGVLRW3nrrrcn8xz/+cTJfunRpXUeCqtGhQ4dkPnny5GT+7rvvFl5j8803r/gcND53MgAAgKyUDAAAICslAwAAyErJAAAAslIyAACArJQMAAAgKyUDAADIyj4ZTcjHH3+czN94440GmgRoLFdeeWUyP/zwwxtoknXXySefnMzvuOOOZP7000/nHAealaI9MGpzjH0yqoM7GQAAQFZKBgAAkJWSAQAAZKVkAAAAWSkZAABAVkoGAACQlZIBAABkZZ+MJqRjx47JfNddd22YQYBG8+STTybzSvfJWLhwYTIv2gOiRYvin03V1NTUaab/rW/fvsm8f//+FZ0faDylUqmxR6CBuJMBAABkpWQAAABZKRkAAEBWSgYAAJCVkgEAAGSlZAAAAFkpGQAAQFb2yWhCNthgg2S+9dZb1/sMe+65ZzKfNWtWMn/jjTdyjgPrnFtuuSWZT5w4saLzf/rpp8n83Xffrej8OWy00UbJfObMmcl8yy23rOj6RZ/jGTNmVHR+WJeVy+XCY9q0adMAk1Df3MkAAACyUjIAAICslAwAACArJQMAAMhKyQAAALJSMgAAgKyUDAAAICv7ZDQh77zzTjK/++67k/nIkSMrnqHoHEuWLEnmY8eOrXgGWJetXr06mb/11lsNNEnjGThwYDLfeOON6/X68+bNS+YrV66s1+vDuq53797J/C9/+UsDTUIl3MkAAACyUjIAAICslAwAACArJQMAAMhKyQAAALJSMgAAgKyUDAAAICv7ZFSRn//858k8xz4ZAPXt+OOPT+bDhg1L5m3bts05zlouu+yyej0/VLOivXw+/PDDZN6hQ4fCa3zjG9+o00w0Te5kAAAAWSkZAABAVkoGAACQlZIBAABkpWQAAABZKRkAAEBWSgYAAJCVfTKakRYt0p2xpqamgSYBmqsTTzyx8JiLLroomXfv3j2Zt2rVqk4z1dVzzz2XzD/99NN6vT5UsyVLliTzp556KpkPGjQo4zQ0Ze5kAAAAWSkZAABAVkoGAACQlZIBAABkpWQAAABZKRkAAEBWSgYAAJCVfTKakaJ9MMrlcgNNAnxV22yzTTL/3ve+l8wPOuigjNOsrV+/foXH1Pdas3Tp0mRetE/HY489lsyXL19e55kA+Dx3MgAAgKyUDAAAICslAwAAyErJAAAAslIyAACArJQMAAAgKyUDAADIyj4ZAA1o5513TuaPPPJIMt96661zjlOVnnrqqWT+r//6rw00CVAfNtlkk8YegQzcyQAAALJSMgAAgKyUDAAAICslAwAAyErJAAAAslIyAACArJQMAAAgKyUDAADIymZ8AE1IqVSqKK9vLVoU/2yqpqamXmcYNGhQMj/ssMOS+eOPP55zHCCzwYMHN/YIZOBOBgAAkJWSAQAAZKVkAAAAWSkZAABAVkoGAACQlZIBAABkpWQAAABZ2SejGSl6fn2OZ9fvt99+yXzs2LEVXwOas5kzZybzAQMGJPOTTjopmT/xxBPJfMWKFcm8IZx66qnJ/KyzzmqgSYDcJk+enMyL9rmh+XAnAwAAyErJAAAAslIyAACArJQMAAAgKyUDAADISskAAACyUjIAAICsSuVyuVyrA0ul+p6FCv3tb39L5rX8n7oiu+yySzJ/8cUX632G5q4h/nesL9YRIiI6dOiQzN97772Kzn/kkUcm88cff7yi8zcH1bqOWEOavqOPPjqZ//u//3vhOZYvX57Md9ppp2T+xhtvFF6DytRmDXEnAwAAyErJAAAAslIyAACArJQMAAAgKyUDAADISskAAACyUjIAAICs1mvsAcjn1ltvTeY//OEP632G0047LZn/+Mc/rvcZgKZt4MCBjT0CUE9Wr15d8TmK9kNp3bp1xdeg/rmTAQAAZKVkAAAAWSkZAABAVkoGAACQlZIBAABkpWQAAABZKRkAAEBW9sloRmbNmtXYI0Cz16pVq2R+yCGHJPNJkyYl8+XLl9d5pqbm+9//fjK/8cYbG2gSoKE9/PDDybw236vsuOOOybxoz60zzjij8BrUP3cyAACArJQMAAAgKyUDAADISskAAACyUjIAAICslAwAACArJQMAAMiqVC6Xy7U6sFSq71moZ6+88krhMd/4xjcqukaLFune2r1792Q+Z86ciq6/Lqjll2yTVA3rSL9+/ZL5T3/602R+8MEHJ/Ntt902mb/11lvJvL516tQpmR9++OGF5xgzZkwyb9++fZ1m+t+K9hIZPHhwMp88eXJF128OqnUdqYY1hLQbbrih8JiivXY222yzZL5ixYq6jMRXUJs1xJ0MAAAgKyUDAADISskAAACyUjIAAICslAwAACArJQMAAMhKyQAAALJar7EHoOG88MILhcd069atomvU1NRU9H5obGPHjk3mO++8c0Xnv+CCC5L5smXLKjp/pYr2+dh9990Lz1HpHgxTpkxJ5rfccksytw8GVLeiNWTVqlUNNAmVcCcDAADISskAAACyUjIAAICslAwAACArJQMAAMhKyQAAALJSMgAAgKzsk7EO+dd//dfCY4488sgGmATWXaeffnpjj1DvFi5cmMx/85vfJPPhw4cn8xUrVtR5JqB6bLTRRsn8W9/6VjKfMGFCznH4itzJAAAAslIyAACArJQMAAAgKyUDAADISskAAACyUjIAAICslAwAACAr+2SsQ1588cXCY1566aVk3qNHj1zjQJM0dOjQZH7WWWcl81NOOSXjNPnNmTMnmX/yySfJ/Kmnniq8RtGePDNnziw8B9A8HXvssYXHrFy5MpkXfa9C0+BOBgAAkJWSAQAAZKVkAAAAWSkZAABAVkoGAACQlZIBAABkpWQAAABZKRkAAEBWpXK5XK7VgaVSfc8C1EItv2SbpOawjrRu3TqZF23m94tf/CKZb7zxxsl84sSJyfzJJ59M5g8//HAyf/fdd5M5zUO1riPNYQ1Z191///2FxxRt/Dt48OBk/sYbb9RpJuquNmuIOxkAAEBWSgYAAJCVkgEAAGSlZAAAAFkpGQAAQFZKBgAAkJWSAQAAZGWfDKgy1fp8+wjrCDQV1bqOWEOgabBPBgAA0OCUDAAAICslAwAAyErJAAAAslIyAACArJQMAAAgKyUDAADISskAAACyUjIAAICslAwAACArJQMAAMhKyQAAALJSMgAAgKyUDAAAICslAwAAyErJAAAAslIyAACArJQMAAAgKyUDAADISskAAACyUjIAAICslAwAACArJQMAAMiqVC6Xy409BAAA0Hy4kwEAAGSlZAAAAFkpGQAAQFZKBgAAkJWSAQAAZKVkAAAAWSkZAABAVkoGAACQlZIBAABk9f8BLKOab8n40dYAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 1000x1000 with 9 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Read and preprocess MNIST Dataset\n",
        "with open(train_image_path, \"rb\") as img_file:\n",
        "    img_data = np.frombuffer(img_file.read(), dtype = np.uint8)\n",
        "\n",
        "with open(train_labels_path, \"rb\") as lbl_file:\n",
        "    label_file = np.frombuffer(lbl_file.read(), dtype = np.uint8)\n",
        "\n",
        "labels = label_file[8:]\n",
        "print(\"Labels Shape: \", labels.shape)\n",
        "\n",
        "images = img_data[16:].reshape(-1, img_dim, img_dim)\n",
        "print(\"Images Shape: \", images.shape)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(9):\n",
        "    plt.subplot(3, 3, i+1)\n",
        "    plt.imshow(images[i], cmap = \"gray\")\n",
        "    plt.title(f\"Label: {labels[i]}\")\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jb4Yvsw6X7zB",
        "outputId": "e06283f1-347c-42b7-f4ab-d7d898469aa2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Data - Images Shape: (48000, 784) Labels Shape: (48000, 10)\n",
            "Validation Data - Images Shape: (12000, 784) Labels Shape: (12000, 10)\n",
            "Test Data - Images Shape: (10000, 784) Labels Shape: (10000, 10)\n"
          ]
        }
      ],
      "source": [
        "# Dataset Loader and Pre-Processing Functions\n",
        "def randomRotation(images, max_angle = 15):\n",
        "    # Function to Randomly Rotate Images\n",
        "    rotated_images = []\n",
        "\n",
        "    for image in images:\n",
        "        decision = np.random.rand()                       # Random Probability\n",
        "\n",
        "        # Apply Pre-Processing (or Rotate) with 50% Probability\n",
        "        if decision < 0.5:\n",
        "            rotated_images.append(image)\n",
        "        else:\n",
        "            angle = np.random.uniform(-max_angle, max_angle)\n",
        "            rotated_image = rotate(image, angle, reshape = False, mode = \"nearest\")\n",
        "            rotated_images.append(rotated_image)\n",
        "\n",
        "    return np.array(rotated_images)\n",
        "\n",
        "def horizontalFlip(images):\n",
        "    # Function to Randomly Flip Images Horizontally\n",
        "    flipped_images = []\n",
        "\n",
        "    for image in images:\n",
        "        decision = np.random.rand()\n",
        "\n",
        "        # Apply Pre-Processing with 50% Probability\n",
        "        if decision < 0.5:\n",
        "            flipped_images.append(image)\n",
        "        else:\n",
        "            flipped_images.append(np.fliplr(image))\n",
        "\n",
        "    return np.array(flipped_images)\n",
        "\n",
        "class MNIST_DataLoader:\n",
        "    def __init__(self, train_image_path, train_labels_path, test_image_path, test_labels_path, img_dim, preprocessors = None):\n",
        "        self.train_image_path = train_image_path\n",
        "        self.train_labels_path = train_labels_path\n",
        "        self.test_image_path = test_image_path\n",
        "        self.test_labels_path = test_labels_path\n",
        "        self.img_dim = img_dim\n",
        "        self.preprocessors = preprocessors if preprocessors is not None else []\n",
        "\n",
        "    def readImageLabels(self, image_path, label_path):\n",
        "        with open(image_path, \"rb\") as file:\n",
        "            images = np.frombuffer(file.read(), dtype = np.uint8)\n",
        "\n",
        "        images = images[16:].reshape(-1, self.img_dim, self.img_dim).astype(np.float32)    # Reshape\n",
        "        images = images / 255.0                                                            # Normalize to Range [0, 1]\n",
        "\n",
        "        # Apply Pre-Processing\n",
        "        for preprocessor in self.preprocessors:\n",
        "            images = preprocessor(images)\n",
        "\n",
        "        with open(label_path, \"rb\") as file:\n",
        "            labels = np.frombuffer(file.read(), dtype = np.uint8)\n",
        "\n",
        "        # One-Hot Encode Labels\n",
        "        one_hot_labels = np.eye(10)[labels[8:]]\n",
        "\n",
        "        return images, one_hot_labels\n",
        "\n",
        "    def loadData(self):\n",
        "        train_images, train_labels = self.readImageLabels(self.train_image_path, self.train_labels_path)\n",
        "        test_images, test_labels = self.readImageLabels(self.test_image_path, self.test_labels_path)\n",
        "\n",
        "        # Flatten Images back to 1D after Pre-Processing\n",
        "        train_images = train_images.reshape(-1, self.img_dim ** 2)\n",
        "        test_images = test_images.reshape(-1, self.img_dim ** 2)\n",
        "\n",
        "        return (train_images, train_labels), (test_images, test_labels)\n",
        "\n",
        "# Creating the DataLoader Object\n",
        "preprocessors = [randomRotation, horizontalFlip]\n",
        "dobj = MNIST_DataLoader(train_image_path, train_labels_path, test_image_path, test_labels_path, img_dim, preprocessors)\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = dobj.loadData()\n",
        "\n",
        "# Splitting the Data into Training and Validation Sets\n",
        "split_ratio = 0.2\n",
        "split_index = int(len(train_images) * split_ratio)\n",
        "train_images, train_labels = train_images[split_index:], train_labels[split_index:]\n",
        "val_images, val_labels = train_images[:split_index], train_labels[:split_index]\n",
        "\n",
        "# Printing the Shapes along with other details for Training, Validation and Test Datasets\n",
        "print(f\"Training Data - Images Shape: {train_images.shape} Labels Shape: {train_labels.shape}\")\n",
        "print(f\"Validation Data - Images Shape: {val_images.shape} Labels Shape: {val_labels.shape}\")\n",
        "print(f\"Test Data - Images Shape: {test_images.shape} Labels Shape: {test_labels.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRx0VnOIYA_E"
      },
      "outputs": [],
      "source": [
        "# Fully Connected Neural Network Implementation\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, layers, dropout_rate = 0.0):\n",
        "        self.layers = layers\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        self.best_loss = float('inf')             # Initialize with a High value\n",
        "        self.best_weights = None\n",
        "        self.best_biases = None\n",
        "\n",
        "        # Initialize Weights and Biases using He Initialization\n",
        "        for i in range(len(layers) - 1):\n",
        "            self.weights.append(np.random.randn(layers[i], layers[i + 1]) * np.sqrt(2 / layers[i]))\n",
        "            self.biases.append(np.zeros((1, layers[i + 1])))\n",
        "\n",
        "    # ReLU Activation Function\n",
        "    def relu(self, z):\n",
        "        return np.maximum(0, z)\n",
        "\n",
        "    # Derivative of ReLU function\n",
        "    def reluDerivative(self, z):\n",
        "        return np.where(z > 0, 1, 0)\n",
        "\n",
        "    # Softmax Activation Function with Stability Improvements\n",
        "    def softmax(self, z):\n",
        "        z_stable = z - np.max(z, axis = 1, keepdims = True)        # Stability Improvement\n",
        "        exp_z = np.exp(z_stable)\n",
        "        return exp_z / np.sum(exp_z, axis = 1, keepdims = True)\n",
        "\n",
        "    # Apply Dropout to Activations during Training\n",
        "    def applyDropout(self, activations, dropout_rate):\n",
        "        if dropout_rate > 0:\n",
        "            assert 0 <= dropout_rate < 1.0, \"Dropout rate must be between 0 and 1 (Exclusive).\"\n",
        "            mask = np.random.rand(*activations.shape) > dropout_rate\n",
        "            activations *= mask\n",
        "            activations /= (1 - dropout_rate)                  # Scale to maintain consistency\n",
        "            return activations, mask\n",
        "        return activations, None\n",
        "\n",
        "    # Perform Feedforward Propagation\n",
        "    def feedForward(self, X, training = True):\n",
        "        activations = [X]\n",
        "        self.dropout_masks = []                         # Store Dropout Masks for Backpropagation\n",
        "        for i in range(len(self.weights)):\n",
        "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
        "            if i == len(self.weights) - 1:              # Output Layer\n",
        "                a = self.softmax(z)\n",
        "            else:                                       # Hidden Layers\n",
        "                a = self.relu(z)\n",
        "                if training:                            # Apply Dropout during Training\n",
        "                    a, mask = self.applyDropout(a, self.dropout_rate)\n",
        "                    self.dropout_masks.append(mask)\n",
        "                else:\n",
        "                    self.dropout_masks.append(None)     # No Dropout during Inference\n",
        "            activations.append(a)\n",
        "        return activations\n",
        "\n",
        "    # Perform Backpropagation to Update Weights and Biases\n",
        "    def backPropagation(self, X, y, learning_rate):\n",
        "        activations = self.feedForward(X, training = True)\n",
        "        deltas = [activations[-1] - y]\n",
        "\n",
        "        for i in range(len(self.layers) - 2, 0, -1):\n",
        "            delta = np.dot(deltas[-1], self.weights[i].T)\n",
        "            if self.dropout_masks[i - 1] is not None:\n",
        "                delta *= self.dropout_masks[i - 1]               # Apply Dropout Mask\n",
        "            delta *= self.reluDerivative(activations[i])\n",
        "            deltas.append(delta)\n",
        "\n",
        "        deltas.reverse()\n",
        "\n",
        "        for i in range(len(self.weights)):\n",
        "            # Update Weights and Biases with Gradient Clipping\n",
        "            grad_w = np.dot(activations[i].T, deltas[i])\n",
        "            grad_b = np.sum(deltas[i], axis = 0, keepdims = True)\n",
        "            grad_w = np.clip(grad_w, -1.5, 1.5)                  # Clip Gradients to Prevent Explosion\n",
        "            grad_b = np.clip(grad_b, -1.5, 1.5)\n",
        "            self.weights[i] -= learning_rate * grad_w\n",
        "            self.biases[i] -= learning_rate * grad_b\n",
        "\n",
        "    # Train the Neural Network\n",
        "    def train(self, X, y, X_val, y_val, epochs, learning_rate):\n",
        "        for epoch in range(epochs):\n",
        "            self.backPropagation(X, y, learning_rate)\n",
        "            train_loss = self.calculateLoss(X, y)\n",
        "            val_loss = self.calculateLoss(X_val, y_val)\n",
        "            accuracy = self.calculateAccuracy(X_val, y_val)\n",
        "\n",
        "            # Save the best Weights based on Validation Loss\n",
        "            if val_loss < self.best_loss:\n",
        "                self.best_loss = val_loss\n",
        "                self.best_weights = [w.copy() for w in self.weights]\n",
        "                self.best_biases = [b.copy() for b in self.biases]\n",
        "                print(f\"Epoch {epoch}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "                self.save_best_weights('bestWeights.npy', self.best_weights, self.best_biases)\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Predict Outputs for the Input Data\n",
        "    def predict(self, X):\n",
        "        return self.feedForward(X, training = False)[-1]\n",
        "\n",
        "    # Calculate the Loss for Given Inputs and True Outputs\n",
        "    def calculateLoss(self, X, y):\n",
        "        predictions = self.predict(X)\n",
        "        # Add Epsilon to Avoid log(0)\n",
        "        return -np.mean(np.sum(y * np.log(predictions + 1e-8), axis = 1))\n",
        "\n",
        "    # Calculate Accuracy for the given Inputs and True Outputs\n",
        "    def calculateAccuracy(self, X, y):\n",
        "        predictions = self.predict(X)\n",
        "        predicted_labels = np.argmax(predictions, axis = 1)\n",
        "        true_labels = np.argmax(y, axis = 1)\n",
        "        return np.mean(predicted_labels == true_labels)\n",
        "\n",
        "    # Save the Best Weights and Biases to a Aile\n",
        "    def save_best_weights(self, filepath, best_weights, best_biases):\n",
        "        np.save(filepath, {'weights': best_weights, 'biases': best_biases})\n",
        "        print(\"Best Weights saved to Disk.\")\n",
        "\n",
        "    # Load Weights and Biases from a File\n",
        "    def loadWeights(self, filepath):\n",
        "        data = np.load(filepath, allow_pickle = True).item()\n",
        "        self.weights = data['weights']\n",
        "        self.biases = data['biases']\n",
        "        print(\"Best Weights loaded from Disk.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlY0dQrOYEjL",
        "outputId": "1412e075-1d7f-4484-bd8a-baea3ce0731e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Trainable Parameters: 848128\n",
            "Epoch 0, Training Loss: 1.9218, Validation Loss: 1.9253, Accuracy: 0.3028\n",
            "Best Weights saved to Disk.\n",
            "Epoch 0, Training Loss: 1.9218, Validation Loss: 1.9253, Accuracy: 0.3028\n",
            "Epoch 8, Training Loss: 1.5442, Validation Loss: 1.5684, Accuracy: 0.5247\n",
            "Best Weights saved to Disk.\n",
            "Epoch 10, Training Loss: 1.3772, Validation Loss: 1.4044, Accuracy: 0.5850\n",
            "Best Weights saved to Disk.\n",
            "Epoch 20, Training Loss: 1.3505, Validation Loss: 1.3755, Accuracy: 0.5665\n",
            "Best Weights saved to Disk.\n",
            "Epoch 22, Training Loss: 1.3182, Validation Loss: 1.3422, Accuracy: 0.5725\n",
            "Best Weights saved to Disk.\n",
            "Epoch 24, Training Loss: 1.1743, Validation Loss: 1.1958, Accuracy: 0.6247\n",
            "Best Weights saved to Disk.\n",
            "Epoch 31, Training Loss: 1.1575, Validation Loss: 1.1522, Accuracy: 0.6605\n",
            "Best Weights saved to Disk.\n",
            "Epoch 40, Training Loss: 1.1366, Validation Loss: 1.1375, Accuracy: 0.6712\n",
            "Best Weights saved to Disk.\n",
            "Epoch 42, Training Loss: 1.0613, Validation Loss: 1.0643, Accuracy: 0.6869\n",
            "Best Weights saved to Disk.\n",
            "Epoch 44, Training Loss: 0.9934, Validation Loss: 0.9964, Accuracy: 0.7066\n",
            "Best Weights saved to Disk.\n",
            "Epoch 46, Training Loss: 0.9647, Validation Loss: 0.9708, Accuracy: 0.7129\n",
            "Best Weights saved to Disk.\n",
            "Epoch 48, Training Loss: 0.9467, Validation Loss: 0.9520, Accuracy: 0.7080\n",
            "Best Weights saved to Disk.\n",
            "Epoch 50, Training Loss: 0.8570, Validation Loss: 0.8503, Accuracy: 0.7316\n",
            "Best Weights saved to Disk.\n",
            "Epoch 72, Training Loss: 0.8590, Validation Loss: 0.8487, Accuracy: 0.7624\n",
            "Best Weights saved to Disk.\n",
            "Epoch 76, Training Loss: 0.8362, Validation Loss: 0.8147, Accuracy: 0.7660\n",
            "Best Weights saved to Disk.\n",
            "Epoch 91, Training Loss: 0.8009, Validation Loss: 0.7913, Accuracy: 0.7627\n",
            "Best Weights saved to Disk.\n",
            "Epoch 100, Training Loss: 0.9074, Validation Loss: 0.8807, Accuracy: 0.7688\n",
            "Epoch 112, Training Loss: 0.7879, Validation Loss: 0.7700, Accuracy: 0.7860\n",
            "Best Weights saved to Disk.\n",
            "Epoch 118, Training Loss: 0.7412, Validation Loss: 0.7477, Accuracy: 0.7938\n",
            "Best Weights saved to Disk.\n",
            "Epoch 120, Training Loss: 0.6945, Validation Loss: 0.6994, Accuracy: 0.8031\n",
            "Best Weights saved to Disk.\n",
            "Epoch 122, Training Loss: 0.6481, Validation Loss: 0.6511, Accuracy: 0.8134\n",
            "Best Weights saved to Disk.\n",
            "Epoch 124, Training Loss: 0.6179, Validation Loss: 0.6201, Accuracy: 0.8247\n",
            "Best Weights saved to Disk.\n",
            "Epoch 136, Training Loss: 0.6311, Validation Loss: 0.6113, Accuracy: 0.8273\n",
            "Best Weights saved to Disk.\n",
            "Epoch 138, Training Loss: 0.6015, Validation Loss: 0.5894, Accuracy: 0.8368\n",
            "Best Weights saved to Disk.\n",
            "Epoch 140, Training Loss: 0.5821, Validation Loss: 0.5725, Accuracy: 0.8400\n",
            "Best Weights saved to Disk.\n",
            "Epoch 142, Training Loss: 0.5726, Validation Loss: 0.5624, Accuracy: 0.8425\n",
            "Best Weights saved to Disk.\n",
            "Epoch 144, Training Loss: 0.5652, Validation Loss: 0.5520, Accuracy: 0.8435\n",
            "Best Weights saved to Disk.\n",
            "Epoch 162, Training Loss: 0.5698, Validation Loss: 0.5450, Accuracy: 0.8522\n",
            "Best Weights saved to Disk.\n",
            "Epoch 164, Training Loss: 0.5491, Validation Loss: 0.5235, Accuracy: 0.8592\n",
            "Best Weights saved to Disk.\n",
            "Epoch 200, Training Loss: 0.6838, Validation Loss: 0.6792, Accuracy: 0.8291\n",
            "Epoch 223, Training Loss: 0.5019, Validation Loss: 0.4906, Accuracy: 0.8712\n",
            "Best Weights saved to Disk.\n",
            "Epoch 245, Training Loss: 0.4775, Validation Loss: 0.4872, Accuracy: 0.8658\n",
            "Best Weights saved to Disk.\n",
            "Epoch 247, Training Loss: 0.4555, Validation Loss: 0.4633, Accuracy: 0.8714\n",
            "Best Weights saved to Disk.\n",
            "Epoch 249, Training Loss: 0.4502, Validation Loss: 0.4500, Accuracy: 0.8750\n",
            "Best Weights saved to Disk.\n",
            "Epoch 270, Training Loss: 0.4556, Validation Loss: 0.4421, Accuracy: 0.8786\n",
            "Best Weights saved to Disk.\n",
            "Epoch 278, Training Loss: 0.4199, Validation Loss: 0.4066, Accuracy: 0.8836\n",
            "Best Weights saved to Disk.\n",
            "Epoch 280, Training Loss: 0.3998, Validation Loss: 0.3874, Accuracy: 0.8877\n",
            "Best Weights saved to Disk.\n",
            "Epoch 290, Training Loss: 0.3825, Validation Loss: 0.3794, Accuracy: 0.8819\n",
            "Best Weights saved to Disk.\n",
            "Epoch 292, Training Loss: 0.3603, Validation Loss: 0.3575, Accuracy: 0.8882\n",
            "Best Weights saved to Disk.\n",
            "Epoch 294, Training Loss: 0.3383, Validation Loss: 0.3392, Accuracy: 0.8925\n",
            "Best Weights saved to Disk.\n",
            "Epoch 296, Training Loss: 0.3178, Validation Loss: 0.3199, Accuracy: 0.8989\n",
            "Best Weights saved to Disk.\n",
            "Epoch 300, Training Loss: 0.4026, Validation Loss: 0.3977, Accuracy: 0.8845\n",
            "Epoch 400, Training Loss: 0.3945, Validation Loss: 0.3822, Accuracy: 0.8898\n",
            "Epoch 403, Training Loss: 0.3263, Validation Loss: 0.3187, Accuracy: 0.9178\n",
            "Best Weights saved to Disk.\n",
            "Epoch 423, Training Loss: 0.3014, Validation Loss: 0.2963, Accuracy: 0.9185\n",
            "Best Weights saved to Disk.\n",
            "Epoch 425, Training Loss: 0.2914, Validation Loss: 0.2850, Accuracy: 0.9215\n",
            "Best Weights saved to Disk.\n",
            "Epoch 461, Training Loss: 0.2785, Validation Loss: 0.2796, Accuracy: 0.9244\n",
            "Best Weights saved to Disk.\n",
            "Epoch 465, Training Loss: 0.2758, Validation Loss: 0.2752, Accuracy: 0.9261\n",
            "Best Weights saved to Disk.\n",
            "Epoch 500, Training Loss: 0.3368, Validation Loss: 0.3328, Accuracy: 0.9193\n",
            "Epoch 511, Training Loss: 0.2789, Validation Loss: 0.2700, Accuracy: 0.9244\n",
            "Best Weights saved to Disk.\n",
            "Epoch 513, Training Loss: 0.2745, Validation Loss: 0.2649, Accuracy: 0.9278\n",
            "Best Weights saved to Disk.\n",
            "Epoch 600, Training Loss: 0.3073, Validation Loss: 0.2974, Accuracy: 0.9282\n",
            "Epoch 604, Training Loss: 0.2744, Validation Loss: 0.2625, Accuracy: 0.9367\n",
            "Best Weights saved to Disk.\n",
            "Epoch 605, Training Loss: 0.2578, Validation Loss: 0.2556, Accuracy: 0.9388\n",
            "Best Weights saved to Disk.\n",
            "Epoch 606, Training Loss: 0.2673, Validation Loss: 0.2538, Accuracy: 0.9379\n",
            "Best Weights saved to Disk.\n",
            "Epoch 607, Training Loss: 0.2547, Validation Loss: 0.2520, Accuracy: 0.9388\n",
            "Best Weights saved to Disk.\n",
            "Epoch 617, Training Loss: 0.2529, Validation Loss: 0.2509, Accuracy: 0.9382\n",
            "Best Weights saved to Disk.\n",
            "Epoch 645, Training Loss: 0.2432, Validation Loss: 0.2464, Accuracy: 0.9417\n",
            "Best Weights saved to Disk.\n",
            "Epoch 679, Training Loss: 0.2543, Validation Loss: 0.2450, Accuracy: 0.9359\n",
            "Best Weights saved to Disk.\n",
            "Epoch 686, Training Loss: 0.2436, Validation Loss: 0.2422, Accuracy: 0.9403\n",
            "Best Weights saved to Disk.\n",
            "Epoch 688, Training Loss: 0.2412, Validation Loss: 0.2389, Accuracy: 0.9393\n",
            "Best Weights saved to Disk.\n",
            "Epoch 690, Training Loss: 0.2344, Validation Loss: 0.2309, Accuracy: 0.9407\n",
            "Best Weights saved to Disk.\n",
            "Epoch 700, Training Loss: 0.2539, Validation Loss: 0.2564, Accuracy: 0.9393\n",
            "Epoch 713, Training Loss: 0.2244, Validation Loss: 0.2213, Accuracy: 0.9415\n",
            "Best Weights saved to Disk.\n",
            "Epoch 715, Training Loss: 0.2152, Validation Loss: 0.2132, Accuracy: 0.9433\n",
            "Best Weights saved to Disk.\n",
            "Epoch 755, Training Loss: 0.2228, Validation Loss: 0.2122, Accuracy: 0.9517\n",
            "Best Weights saved to Disk.\n",
            "Epoch 757, Training Loss: 0.2130, Validation Loss: 0.2044, Accuracy: 0.9516\n",
            "Best Weights saved to Disk.\n",
            "Epoch 759, Training Loss: 0.2016, Validation Loss: 0.1931, Accuracy: 0.9539\n",
            "Best Weights saved to Disk.\n",
            "Epoch 761, Training Loss: 0.1945, Validation Loss: 0.1863, Accuracy: 0.9557\n",
            "Best Weights saved to Disk.\n",
            "Epoch 763, Training Loss: 0.1906, Validation Loss: 0.1830, Accuracy: 0.9564\n",
            "Best Weights saved to Disk.\n",
            "Epoch 767, Training Loss: 0.1856, Validation Loss: 0.1801, Accuracy: 0.9578\n",
            "Best Weights saved to Disk.\n",
            "Epoch 769, Training Loss: 0.1827, Validation Loss: 0.1797, Accuracy: 0.9559\n",
            "Best Weights saved to Disk.\n",
            "Epoch 800, Training Loss: 0.2481, Validation Loss: 0.2367, Accuracy: 0.9387\n",
            "Epoch 858, Training Loss: 0.1806, Validation Loss: 0.1783, Accuracy: 0.9550\n",
            "Best Weights saved to Disk.\n",
            "Epoch 860, Training Loss: 0.1726, Validation Loss: 0.1722, Accuracy: 0.9564\n",
            "Best Weights saved to Disk.\n",
            "Epoch 862, Training Loss: 0.1658, Validation Loss: 0.1663, Accuracy: 0.9572\n",
            "Best Weights saved to Disk.\n",
            "Epoch 868, Training Loss: 0.1680, Validation Loss: 0.1643, Accuracy: 0.9554\n",
            "Best Weights saved to Disk.\n",
            "Epoch 870, Training Loss: 0.1642, Validation Loss: 0.1597, Accuracy: 0.9576\n",
            "Best Weights saved to Disk.\n",
            "Epoch 900, Training Loss: 0.1961, Validation Loss: 0.1979, Accuracy: 0.9526\n",
            "Epoch 937, Training Loss: 0.1555, Validation Loss: 0.1543, Accuracy: 0.9561\n",
            "Best Weights saved to Disk.\n",
            "Epoch 939, Training Loss: 0.1480, Validation Loss: 0.1475, Accuracy: 0.9583\n",
            "Best Weights saved to Disk.\n",
            "Epoch 982, Training Loss: 0.1442, Validation Loss: 0.1422, Accuracy: 0.9637\n",
            "Best Weights saved to Disk.\n",
            "Epoch 984, Training Loss: 0.1396, Validation Loss: 0.1369, Accuracy: 0.9650\n",
            "Best Weights saved to Disk.\n",
            "Epoch 986, Training Loss: 0.1370, Validation Loss: 0.1357, Accuracy: 0.9653\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1000, Training Loss: 0.1534, Validation Loss: 0.1468, Accuracy: 0.9629\n",
            "Epoch 1017, Training Loss: 0.1331, Validation Loss: 0.1327, Accuracy: 0.9669\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1019, Training Loss: 0.1313, Validation Loss: 0.1315, Accuracy: 0.9658\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1021, Training Loss: 0.1299, Validation Loss: 0.1305, Accuracy: 0.9663\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1037, Training Loss: 0.1294, Validation Loss: 0.1283, Accuracy: 0.9676\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1041, Training Loss: 0.1243, Validation Loss: 0.1242, Accuracy: 0.9683\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1074, Training Loss: 0.1204, Validation Loss: 0.1224, Accuracy: 0.9688\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1085, Training Loss: 0.1254, Validation Loss: 0.1222, Accuracy: 0.9689\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1087, Training Loss: 0.1237, Validation Loss: 0.1212, Accuracy: 0.9683\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1088, Training Loss: 0.1187, Validation Loss: 0.1187, Accuracy: 0.9680\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1089, Training Loss: 0.1215, Validation Loss: 0.1186, Accuracy: 0.9687\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1090, Training Loss: 0.1153, Validation Loss: 0.1159, Accuracy: 0.9696\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1092, Training Loss: 0.1123, Validation Loss: 0.1134, Accuracy: 0.9703\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1094, Training Loss: 0.1109, Validation Loss: 0.1128, Accuracy: 0.9699\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1096, Training Loss: 0.1057, Validation Loss: 0.1078, Accuracy: 0.9712\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1098, Training Loss: 0.1039, Validation Loss: 0.1076, Accuracy: 0.9725\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1100, Training Loss: 0.1054, Validation Loss: 0.1108, Accuracy: 0.9730\n",
            "Epoch 1152, Training Loss: 0.1107, Validation Loss: 0.1025, Accuracy: 0.9718\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1160, Training Loss: 0.0999, Validation Loss: 0.1019, Accuracy: 0.9728\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1162, Training Loss: 0.0912, Validation Loss: 0.0920, Accuracy: 0.9738\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1164, Training Loss: 0.0857, Validation Loss: 0.0863, Accuracy: 0.9754\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1166, Training Loss: 0.0831, Validation Loss: 0.0840, Accuracy: 0.9762\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1168, Training Loss: 0.0810, Validation Loss: 0.0821, Accuracy: 0.9762\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1170, Training Loss: 0.0804, Validation Loss: 0.0815, Accuracy: 0.9772\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1172, Training Loss: 0.0790, Validation Loss: 0.0801, Accuracy: 0.9772\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1174, Training Loss: 0.0773, Validation Loss: 0.0782, Accuracy: 0.9775\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1176, Training Loss: 0.0770, Validation Loss: 0.0779, Accuracy: 0.9772\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1200, Training Loss: 0.1074, Validation Loss: 0.1008, Accuracy: 0.9731\n",
            "Epoch 1300, Training Loss: 0.1302, Validation Loss: 0.1290, Accuracy: 0.9657\n",
            "Epoch 1305, Training Loss: 0.0823, Validation Loss: 0.0768, Accuracy: 0.9773\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1307, Training Loss: 0.0790, Validation Loss: 0.0750, Accuracy: 0.9780\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1313, Training Loss: 0.0716, Validation Loss: 0.0704, Accuracy: 0.9775\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1315, Training Loss: 0.0706, Validation Loss: 0.0699, Accuracy: 0.9788\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1317, Training Loss: 0.0691, Validation Loss: 0.0682, Accuracy: 0.9792\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1378, Training Loss: 0.0698, Validation Loss: 0.0675, Accuracy: 0.9795\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1380, Training Loss: 0.0681, Validation Loss: 0.0657, Accuracy: 0.9799\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1382, Training Loss: 0.0662, Validation Loss: 0.0648, Accuracy: 0.9805\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1384, Training Loss: 0.0633, Validation Loss: 0.0635, Accuracy: 0.9818\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1390, Training Loss: 0.0617, Validation Loss: 0.0622, Accuracy: 0.9811\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1400, Training Loss: 0.0842, Validation Loss: 0.0833, Accuracy: 0.9746\n",
            "Epoch 1496, Training Loss: 0.0642, Validation Loss: 0.0595, Accuracy: 0.9832\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1498, Training Loss: 0.0619, Validation Loss: 0.0572, Accuracy: 0.9842\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1500, Training Loss: 0.0598, Validation Loss: 0.0548, Accuracy: 0.9850\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1500, Training Loss: 0.0598, Validation Loss: 0.0548, Accuracy: 0.9850\n",
            "Epoch 1502, Training Loss: 0.0587, Validation Loss: 0.0542, Accuracy: 0.9851\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1504, Training Loss: 0.0549, Validation Loss: 0.0513, Accuracy: 0.9858\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1506, Training Loss: 0.0542, Validation Loss: 0.0508, Accuracy: 0.9867\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1545, Training Loss: 0.0508, Validation Loss: 0.0502, Accuracy: 0.9861\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1547, Training Loss: 0.0493, Validation Loss: 0.0479, Accuracy: 0.9867\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1549, Training Loss: 0.0478, Validation Loss: 0.0459, Accuracy: 0.9867\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1551, Training Loss: 0.0460, Validation Loss: 0.0444, Accuracy: 0.9869\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1553, Training Loss: 0.0452, Validation Loss: 0.0440, Accuracy: 0.9869\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1555, Training Loss: 0.0434, Validation Loss: 0.0425, Accuracy: 0.9878\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1557, Training Loss: 0.0427, Validation Loss: 0.0417, Accuracy: 0.9882\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1559, Training Loss: 0.0414, Validation Loss: 0.0408, Accuracy: 0.9876\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1561, Training Loss: 0.0418, Validation Loss: 0.0405, Accuracy: 0.9876\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1563, Training Loss: 0.0419, Validation Loss: 0.0402, Accuracy: 0.9878\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1565, Training Loss: 0.0420, Validation Loss: 0.0402, Accuracy: 0.9882\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1600, Training Loss: 0.0593, Validation Loss: 0.0587, Accuracy: 0.9822\n",
            "Epoch 1638, Training Loss: 0.0411, Validation Loss: 0.0397, Accuracy: 0.9878\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1640, Training Loss: 0.0397, Validation Loss: 0.0385, Accuracy: 0.9883\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1642, Training Loss: 0.0378, Validation Loss: 0.0366, Accuracy: 0.9894\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1644, Training Loss: 0.0377, Validation Loss: 0.0361, Accuracy: 0.9894\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1700, Training Loss: 0.0492, Validation Loss: 0.0490, Accuracy: 0.9848\n",
            "Epoch 1710, Training Loss: 0.0356, Validation Loss: 0.0356, Accuracy: 0.9898\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1712, Training Loss: 0.0344, Validation Loss: 0.0339, Accuracy: 0.9900\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1714, Training Loss: 0.0327, Validation Loss: 0.0317, Accuracy: 0.9911\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1716, Training Loss: 0.0316, Validation Loss: 0.0309, Accuracy: 0.9905\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1765, Training Loss: 0.0322, Validation Loss: 0.0307, Accuracy: 0.9908\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1779, Training Loss: 0.0305, Validation Loss: 0.0294, Accuracy: 0.9910\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1800, Training Loss: 0.0546, Validation Loss: 0.0560, Accuracy: 0.9849\n",
            "Epoch 1838, Training Loss: 0.0303, Validation Loss: 0.0286, Accuracy: 0.9919\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1840, Training Loss: 0.0294, Validation Loss: 0.0279, Accuracy: 0.9918\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1842, Training Loss: 0.0281, Validation Loss: 0.0261, Accuracy: 0.9922\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1900, Training Loss: 0.0283, Validation Loss: 0.0285, Accuracy: 0.9930\n",
            "Epoch 1939, Training Loss: 0.0269, Validation Loss: 0.0261, Accuracy: 0.9917\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1968, Training Loss: 0.0273, Validation Loss: 0.0257, Accuracy: 0.9924\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1970, Training Loss: 0.0264, Validation Loss: 0.0251, Accuracy: 0.9925\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1972, Training Loss: 0.0260, Validation Loss: 0.0245, Accuracy: 0.9926\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1974, Training Loss: 0.0253, Validation Loss: 0.0237, Accuracy: 0.9932\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1976, Training Loss: 0.0248, Validation Loss: 0.0231, Accuracy: 0.9931\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1980, Training Loss: 0.0241, Validation Loss: 0.0228, Accuracy: 0.9937\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1982, Training Loss: 0.0235, Validation Loss: 0.0220, Accuracy: 0.9935\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1984, Training Loss: 0.0229, Validation Loss: 0.0213, Accuracy: 0.9939\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1986, Training Loss: 0.0222, Validation Loss: 0.0205, Accuracy: 0.9941\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1988, Training Loss: 0.0220, Validation Loss: 0.0205, Accuracy: 0.9946\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1990, Training Loss: 0.0217, Validation Loss: 0.0203, Accuracy: 0.9945\n",
            "Best Weights saved to Disk.\n",
            "Epoch 1997, Training Loss: 0.0195, Validation Loss: 0.0198, Accuracy: 0.9937\n",
            "Best Weights saved to Disk.\n",
            "Epoch 2000, Training Loss: 0.0336, Validation Loss: 0.0339, Accuracy: 0.9908\n",
            "Epoch 2100, Training Loss: 0.0331, Validation Loss: 0.0321, Accuracy: 0.9906\n",
            "Epoch 2115, Training Loss: 0.0199, Validation Loss: 0.0195, Accuracy: 0.9940\n",
            "Best Weights saved to Disk.\n",
            "Epoch 2117, Training Loss: 0.0190, Validation Loss: 0.0186, Accuracy: 0.9947\n",
            "Best Weights saved to Disk.\n",
            "Epoch 2119, Training Loss: 0.0186, Validation Loss: 0.0180, Accuracy: 0.9944\n",
            "Best Weights saved to Disk.\n",
            "Epoch 2121, Training Loss: 0.0185, Validation Loss: 0.0178, Accuracy: 0.9946\n",
            "Best Weights saved to Disk.\n",
            "Epoch 2123, Training Loss: 0.0162, Validation Loss: 0.0163, Accuracy: 0.9946\n",
            "Best Weights saved to Disk.\n",
            "Epoch 2127, Training Loss: 0.0165, Validation Loss: 0.0162, Accuracy: 0.9951\n",
            "Best Weights saved to Disk.\n",
            "Epoch 2195, Training Loss: 0.0173, Validation Loss: 0.0157, Accuracy: 0.9952\n",
            "Best Weights saved to Disk.\n",
            "Epoch 2197, Training Loss: 0.0165, Validation Loss: 0.0155, Accuracy: 0.9948\n",
            "Best Weights saved to Disk.\n",
            "Epoch 2198, Training Loss: 0.0159, Validation Loss: 0.0154, Accuracy: 0.9952\n",
            "Best Weights saved to Disk.\n",
            "Epoch 2199, Training Loss: 0.0156, Validation Loss: 0.0143, Accuracy: 0.9951\n",
            "Best Weights saved to Disk.\n",
            "Epoch 2200, Training Loss: 0.0150, Validation Loss: 0.0151, Accuracy: 0.9956\n",
            "Epoch 2201, Training Loss: 0.0152, Validation Loss: 0.0141, Accuracy: 0.9954\n",
            "Best Weights saved to Disk.\n",
            "Epoch 2293, Training Loss: 0.0135, Validation Loss: 0.0132, Accuracy: 0.9957\n",
            "Best Weights saved to Disk.\n",
            "Epoch 2299, Training Loss: 0.0132, Validation Loss: 0.0130, Accuracy: 0.9960\n",
            "Best Weights saved to Disk.\n",
            "Epoch 2300, Training Loss: 0.0169, Validation Loss: 0.0158, Accuracy: 0.9948\n",
            "Epoch 2303, Training Loss: 0.0133, Validation Loss: 0.0125, Accuracy: 0.9959\n",
            "Best Weights saved to Disk.\n",
            "Epoch 2307, Training Loss: 0.0126, Validation Loss: 0.0121, Accuracy: 0.9963\n",
            "Best Weights saved to Disk.\n",
            "Epoch 2309, Training Loss: 0.0121, Validation Loss: 0.0114, Accuracy: 0.9963\n",
            "Best Weights saved to Disk.\n",
            "Epoch 2338, Training Loss: 0.0123, Validation Loss: 0.0113, Accuracy: 0.9971\n",
            "Best Weights saved to Disk.\n",
            "Epoch 2340, Training Loss: 0.0125, Validation Loss: 0.0111, Accuracy: 0.9972\n",
            "Best Weights saved to Disk.\n",
            "Epoch 2342, Training Loss: 0.0111, Validation Loss: 0.0102, Accuracy: 0.9975\n",
            "Best Weights saved to Disk.\n",
            "Epoch 2352, Training Loss: 0.0104, Validation Loss: 0.0094, Accuracy: 0.9975\n",
            "Best Weights saved to Disk.\n",
            "Epoch 2358, Training Loss: 0.0104, Validation Loss: 0.0092, Accuracy: 0.9972\n",
            "Best Weights saved to Disk.\n",
            "Epoch 2360, Training Loss: 0.0095, Validation Loss: 0.0084, Accuracy: 0.9973\n",
            "Best Weights saved to Disk.\n",
            "Epoch 2362, Training Loss: 0.0092, Validation Loss: 0.0084, Accuracy: 0.9976\n",
            "Best Weights saved to Disk.\n",
            "Epoch 2364, Training Loss: 0.0090, Validation Loss: 0.0081, Accuracy: 0.9975\n",
            "Best Weights saved to Disk.\n",
            "Epoch 2368, Training Loss: 0.0086, Validation Loss: 0.0081, Accuracy: 0.9969\n",
            "Best Weights saved to Disk.\n",
            "Epoch 2400, Training Loss: 0.0229, Validation Loss: 0.0224, Accuracy: 0.9930\n"
          ]
        }
      ],
      "source": [
        "# Define Network Structure: 784 Input Neurons, 2 Hidden Layers with 256 and 128 Neurons each, and 10 Output Neurons\n",
        "layers = [img_dim ** 2, 256, 128, 10]\n",
        "\n",
        "# Calculate the Number of Trainable Parameters in the Network\n",
        "params = 0\n",
        "for i in range(len(layers) - 1):\n",
        "    if i == 0:\n",
        "        params += layers[i] * img_dim ** 2\n",
        "    else:\n",
        "        params += layers[i] * layers[i - 1]\n",
        "\n",
        "print(\"Total Trainable Parameters:\", params)\n",
        "\n",
        "# Create the Neural Network with a different Learning Rate\n",
        "learning_rate = 0.2\n",
        "nn = NeuralNetwork(layers, learning_rate)\n",
        "\n",
        "# Train the Network (Using train_images and train_labels)\n",
        "nn.train(train_images, train_labels, val_images, val_labels, epochs = 2500, learning_rate = 0.005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zn5uGwjJajQd",
        "outputId": "a7d3d412-c8cc-4c4f-99e3-5b037cf4033b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Weights loaded from Disk.\n",
            "Test Accuracy: 95.79%\n",
            "Test Loss: 0.3020152725470206\n"
          ]
        }
      ],
      "source": [
        "nn.loadWeights('bestWeights.npy')\n",
        "accuracy = nn.calculateAccuracy(test_images, test_labels)\n",
        "loss = nn.calculateLoss(test_images, test_labels)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Test Loss: {loss}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
