{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52eac6c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T18:27:21.586991Z",
     "iopub.status.busy": "2025-02-11T18:27:21.586688Z",
     "iopub.status.idle": "2025-02-11T18:27:21.591196Z",
     "shell.execute_reply": "2025-02-11T18:27:21.590548Z"
    },
    "papermill": {
     "duration": 0.009278,
     "end_time": "2025-02-11T18:27:21.592426",
     "exception": false,
     "start_time": "2025-02-11T18:27:21.583148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict A Poem Using 100 Poem Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21b98acd",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-11T18:27:21.597779Z",
     "iopub.status.busy": "2025-02-11T18:27:21.597549Z",
     "iopub.status.idle": "2025-02-11T18:27:24.594931Z",
     "shell.execute_reply": "2025-02-11T18:27:24.594221Z"
    },
    "papermill": {
     "duration": 3.001569,
     "end_time": "2025-02-11T18:27:24.596480",
     "exception": false,
     "start_time": "2025-02-11T18:27:21.594911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be69bdb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T18:27:24.603545Z",
     "iopub.status.busy": "2025-02-11T18:27:24.603182Z",
     "iopub.status.idle": "2025-02-11T18:27:24.622318Z",
     "shell.execute_reply": "2025-02-11T18:27:24.621512Z"
    },
    "papermill": {
     "duration": 0.023759,
     "end_time": "2025-02-11T18:27:24.623592",
     "exception": false,
     "start_time": "2025-02-11T18:27:24.599833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the Dataset\n",
    "text = \"\"\n",
    "with open(\"/kaggle/input/poems-dataset/poems-100.csv\", \"r\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        text += \" \".join(row) + \" \"                          # Combine All Lines into a Single Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eaeceda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T18:27:24.628220Z",
     "iopub.status.busy": "2025-02-11T18:27:24.628018Z",
     "iopub.status.idle": "2025-02-11T18:27:24.633507Z",
     "shell.execute_reply": "2025-02-11T18:27:24.632881Z"
    },
    "papermill": {
     "duration": 0.009065,
     "end_time": "2025-02-11T18:27:24.634722",
     "exception": false,
     "start_time": "2025-02-11T18:27:24.625657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize the Text into Words\n",
    "tokens = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d020b4d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T18:27:24.639252Z",
     "iopub.status.busy": "2025-02-11T18:27:24.639058Z",
     "iopub.status.idle": "2025-02-11T18:27:24.649841Z",
     "shell.execute_reply": "2025-02-11T18:27:24.649218Z"
    },
    "papermill": {
     "duration": 0.014216,
     "end_time": "2025-02-11T18:27:24.650946",
     "exception": false,
     "start_time": "2025-02-11T18:27:24.636730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Dictionary to Map Words to Indices\n",
    "word_to_idx = {}\n",
    "idx_to_word = {}\n",
    "vocab_size = 0\n",
    "\n",
    "for word in tokens:\n",
    "    if word not in word_to_idx:\n",
    "        word_to_idx[word] = vocab_size\n",
    "        idx_to_word[vocab_size] = word\n",
    "        vocab_size += 1\n",
    "\n",
    "# Convert Tokens to Indices\n",
    "token_indices = [word_to_idx[word] for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04c5f44b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T18:27:24.655516Z",
     "iopub.status.busy": "2025-02-11T18:27:24.655268Z",
     "iopub.status.idle": "2025-02-11T18:27:24.787230Z",
     "shell.execute_reply": "2025-02-11T18:27:24.786621Z"
    },
    "papermill": {
     "duration": 0.135496,
     "end_time": "2025-02-11T18:27:24.788427",
     "exception": false,
     "start_time": "2025-02-11T18:27:24.652931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Sequences and Targets\n",
    "seq_length = 10\n",
    "sequences = []\n",
    "targets = []\n",
    "\n",
    "for i in range(len(token_indices) - seq_length):\n",
    "    seq = token_indices[i:i + seq_length]\n",
    "    target = token_indices[i + seq_length]\n",
    "    sequences.append(seq)\n",
    "    targets.append(target)\n",
    "\n",
    "# Convert to PyTorch Tensors\n",
    "sequences = torch.tensor(sequences, dtype = torch.long)\n",
    "targets = torch.tensor(targets, dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be4cbeaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T18:27:24.793602Z",
     "iopub.status.busy": "2025-02-11T18:27:24.793334Z",
     "iopub.status.idle": "2025-02-11T18:27:24.827799Z",
     "shell.execute_reply": "2025-02-11T18:27:24.826987Z"
    },
    "papermill": {
     "duration": 0.03835,
     "end_time": "2025-02-11T18:27:24.829133",
     "exception": false,
     "start_time": "2025-02-11T18:27:24.790783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PoemRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super(PoemRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        out = self.fc(output[:, -1, :])                    # Use the Last Hidden State for Prediction\n",
    "        return out\n",
    "\n",
    "# Hyperparameters\n",
    "embed_dim = 100\n",
    "hidden_dim = 128\n",
    "output_dim = vocab_size\n",
    "\n",
    "# Initialize the Model\n",
    "model = PoemRNN(vocab_size, embed_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03cb4052",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T18:27:24.834254Z",
     "iopub.status.busy": "2025-02-11T18:27:24.834028Z",
     "iopub.status.idle": "2025-02-11T18:27:28.261775Z",
     "shell.execute_reply": "2025-02-11T18:27:28.261040Z"
    },
    "papermill": {
     "duration": 3.431851,
     "end_time": "2025-02-11T18:27:28.263256",
     "exception": false,
     "start_time": "2025-02-11T18:27:24.831405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea64cb86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T18:27:28.268805Z",
     "iopub.status.busy": "2025-02-11T18:27:28.268434Z",
     "iopub.status.idle": "2025-02-11T19:14:09.173201Z",
     "shell.execute_reply": "2025-02-11T19:14:09.172106Z"
    },
    "papermill": {
     "duration": 2800.908987,
     "end_time": "2025-02-11T19:14:09.174692",
     "exception": false,
     "start_time": "2025-02-11T18:27:28.265705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/250], Loss: 7.1773\n",
      "Epoch [2/250], Loss: 6.3784\n",
      "Epoch [3/250], Loss: 5.5785\n",
      "Epoch [4/250], Loss: 4.9980\n",
      "Epoch [5/250], Loss: 4.5038\n",
      "Epoch [6/250], Loss: 3.8939\n",
      "Epoch [7/250], Loss: 3.3710\n",
      "Epoch [8/250], Loss: 2.9164\n",
      "Epoch [9/250], Loss: 2.4995\n",
      "Epoch [10/250], Loss: 2.1696\n",
      "Epoch [11/250], Loss: 1.9270\n",
      "Epoch [12/250], Loss: 1.6605\n",
      "Epoch [13/250], Loss: 1.5087\n",
      "Epoch [14/250], Loss: 1.3465\n",
      "Epoch [15/250], Loss: 1.2375\n",
      "Epoch [16/250], Loss: 1.2320\n",
      "Epoch [17/250], Loss: 1.0777\n",
      "Epoch [18/250], Loss: 1.0685\n",
      "Epoch [19/250], Loss: 0.7137\n",
      "Epoch [20/250], Loss: 0.8408\n",
      "Epoch [21/250], Loss: 0.7069\n",
      "Epoch [22/250], Loss: 0.6931\n",
      "Epoch [23/250], Loss: 0.5312\n",
      "Epoch [24/250], Loss: 0.5305\n",
      "Epoch [25/250], Loss: 0.3779\n",
      "Epoch [26/250], Loss: 0.5130\n",
      "Epoch [27/250], Loss: 0.5685\n",
      "Epoch [28/250], Loss: 0.7442\n",
      "Epoch [29/250], Loss: 0.6545\n",
      "Epoch [30/250], Loss: 0.4889\n",
      "Epoch [31/250], Loss: 0.2522\n",
      "Epoch [32/250], Loss: 0.2165\n",
      "Epoch [33/250], Loss: 0.1536\n",
      "Epoch [34/250], Loss: 0.1722\n",
      "Epoch [35/250], Loss: 0.1905\n",
      "Epoch [36/250], Loss: 0.0964\n",
      "Epoch [37/250], Loss: 0.0691\n",
      "Epoch [38/250], Loss: 0.0628\n",
      "Epoch [39/250], Loss: 0.0527\n",
      "Epoch [40/250], Loss: 0.0622\n",
      "Epoch [41/250], Loss: 0.0875\n",
      "Epoch [42/250], Loss: 0.0468\n",
      "Epoch [43/250], Loss: 0.0528\n",
      "Epoch [44/250], Loss: 0.0627\n",
      "Epoch [45/250], Loss: 0.0261\n",
      "Epoch [46/250], Loss: 0.0372\n",
      "Epoch [47/250], Loss: 0.0261\n",
      "Epoch [48/250], Loss: 0.0249\n",
      "Epoch [49/250], Loss: 0.0211\n",
      "Epoch [50/250], Loss: 0.0682\n",
      "Epoch [51/250], Loss: 0.0971\n",
      "Epoch [52/250], Loss: 0.0224\n",
      "Epoch [53/250], Loss: 0.0103\n",
      "Epoch [54/250], Loss: 0.0052\n",
      "Epoch [55/250], Loss: 0.0040\n",
      "Epoch [56/250], Loss: 0.0031\n",
      "Epoch [57/250], Loss: 0.0036\n",
      "Epoch [58/250], Loss: 0.6217\n",
      "Epoch [59/250], Loss: 0.1574\n",
      "Epoch [60/250], Loss: 0.0460\n",
      "Epoch [61/250], Loss: 0.0092\n",
      "Epoch [62/250], Loss: 0.0071\n",
      "Epoch [63/250], Loss: 0.0056\n",
      "Epoch [64/250], Loss: 0.0043\n",
      "Epoch [65/250], Loss: 0.0033\n",
      "Epoch [66/250], Loss: 0.0026\n",
      "Epoch [67/250], Loss: 0.0021\n",
      "Epoch [68/250], Loss: 0.0017\n",
      "Epoch [69/250], Loss: 0.0014\n",
      "Epoch [70/250], Loss: 0.3255\n",
      "Epoch [71/250], Loss: 0.0709\n",
      "Epoch [72/250], Loss: 0.0199\n",
      "Epoch [73/250], Loss: 0.0151\n",
      "Epoch [74/250], Loss: 0.0086\n",
      "Epoch [75/250], Loss: 0.0049\n",
      "Epoch [76/250], Loss: 0.0038\n",
      "Epoch [77/250], Loss: 0.0030\n",
      "Epoch [78/250], Loss: 0.0024\n",
      "Epoch [79/250], Loss: 0.0019\n",
      "Epoch [80/250], Loss: 0.0015\n",
      "Epoch [81/250], Loss: 0.0012\n",
      "Epoch [82/250], Loss: 0.0010\n",
      "Epoch [83/250], Loss: 0.0011\n",
      "Epoch [84/250], Loss: 0.6640\n",
      "Epoch [85/250], Loss: 0.0872\n",
      "Epoch [86/250], Loss: 0.0186\n",
      "Epoch [87/250], Loss: 0.0166\n",
      "Epoch [88/250], Loss: 0.0107\n",
      "Epoch [89/250], Loss: 0.0077\n",
      "Epoch [90/250], Loss: 0.0061\n",
      "Epoch [91/250], Loss: 0.0050\n",
      "Epoch [92/250], Loss: 0.0048\n",
      "Epoch [93/250], Loss: 0.7459\n",
      "Epoch [94/250], Loss: 0.0500\n",
      "Epoch [95/250], Loss: 0.0222\n",
      "Epoch [96/250], Loss: 0.0097\n",
      "Epoch [97/250], Loss: 0.0062\n",
      "Epoch [98/250], Loss: 0.0041\n",
      "Epoch [99/250], Loss: 0.0034\n",
      "Epoch [100/250], Loss: 0.0027\n",
      "Epoch [101/250], Loss: 0.0022\n",
      "Epoch [102/250], Loss: 0.0017\n",
      "Epoch [103/250], Loss: 0.0014\n",
      "Epoch [104/250], Loss: 0.0011\n",
      "Epoch [105/250], Loss: 0.0009\n",
      "Epoch [106/250], Loss: 0.0008\n",
      "Epoch [107/250], Loss: 0.5239\n",
      "Epoch [108/250], Loss: 0.1255\n",
      "Epoch [109/250], Loss: 0.0895\n",
      "Epoch [110/250], Loss: 0.0140\n",
      "Epoch [111/250], Loss: 0.0061\n",
      "Epoch [112/250], Loss: 0.0049\n",
      "Epoch [113/250], Loss: 0.0039\n",
      "Epoch [114/250], Loss: 0.0027\n",
      "Epoch [115/250], Loss: 0.0022\n",
      "Epoch [116/250], Loss: 0.0018\n",
      "Epoch [117/250], Loss: 0.0015\n",
      "Epoch [118/250], Loss: 0.0011\n",
      "Epoch [119/250], Loss: 0.0010\n",
      "Epoch [120/250], Loss: 0.0009\n",
      "Epoch [121/250], Loss: 0.4529\n",
      "Epoch [122/250], Loss: 0.0657\n",
      "Epoch [123/250], Loss: 0.0201\n",
      "Epoch [124/250], Loss: 0.0165\n",
      "Epoch [125/250], Loss: 0.0058\n",
      "Epoch [126/250], Loss: 0.0058\n",
      "Epoch [127/250], Loss: 0.0032\n",
      "Epoch [128/250], Loss: 0.0028\n",
      "Epoch [129/250], Loss: 0.4639\n",
      "Epoch [130/250], Loss: 0.2484\n",
      "Epoch [131/250], Loss: 0.0075\n",
      "Epoch [132/250], Loss: 0.0078\n",
      "Epoch [133/250], Loss: 0.0051\n",
      "Epoch [134/250], Loss: 0.0039\n",
      "Epoch [135/250], Loss: 0.0033\n",
      "Epoch [136/250], Loss: 0.0022\n",
      "Epoch [137/250], Loss: 0.0020\n",
      "Epoch [138/250], Loss: 0.6260\n",
      "Epoch [139/250], Loss: 0.0159\n",
      "Epoch [140/250], Loss: 0.0161\n",
      "Epoch [141/250], Loss: 0.0049\n",
      "Epoch [142/250], Loss: 0.0048\n",
      "Epoch [143/250], Loss: 0.0035\n",
      "Epoch [144/250], Loss: 0.0028\n",
      "Epoch [145/250], Loss: 0.0020\n",
      "Epoch [146/250], Loss: 0.0017\n",
      "Epoch [147/250], Loss: 0.0015\n",
      "Epoch [148/250], Loss: 0.0021\n",
      "Epoch [149/250], Loss: 0.1162\n",
      "Epoch [150/250], Loss: 0.0572\n",
      "Epoch [151/250], Loss: 0.0198\n",
      "Epoch [152/250], Loss: 0.0057\n",
      "Epoch [153/250], Loss: 0.0064\n",
      "Epoch [154/250], Loss: 0.0061\n",
      "Epoch [155/250], Loss: 0.0177\n",
      "Epoch [156/250], Loss: 0.0530\n",
      "Epoch [157/250], Loss: 0.1117\n",
      "Epoch [158/250], Loss: 0.0178\n",
      "Epoch [159/250], Loss: 0.0100\n",
      "Epoch [160/250], Loss: 0.0033\n",
      "Epoch [161/250], Loss: 0.0035\n",
      "Epoch [162/250], Loss: 0.0118\n",
      "Epoch [163/250], Loss: 0.2119\n",
      "Epoch [164/250], Loss: 0.0488\n",
      "Epoch [165/250], Loss: 0.0078\n",
      "Epoch [166/250], Loss: 0.0063\n",
      "Epoch [167/250], Loss: 0.0081\n",
      "Epoch [168/250], Loss: 0.0039\n",
      "Epoch [169/250], Loss: 0.0040\n",
      "Epoch [170/250], Loss: 0.0030\n",
      "Epoch [171/250], Loss: 0.0020\n",
      "Epoch [172/250], Loss: 0.3451\n",
      "Epoch [173/250], Loss: 0.1127\n",
      "Epoch [174/250], Loss: 0.0178\n",
      "Epoch [175/250], Loss: 0.0112\n",
      "Epoch [176/250], Loss: 0.0045\n",
      "Epoch [177/250], Loss: 0.0042\n",
      "Epoch [178/250], Loss: 0.0034\n",
      "Epoch [179/250], Loss: 0.0037\n",
      "Epoch [180/250], Loss: 0.0673\n",
      "Epoch [181/250], Loss: 0.0376\n",
      "Epoch [182/250], Loss: 0.0542\n",
      "Epoch [183/250], Loss: 0.0372\n",
      "Epoch [184/250], Loss: 0.0036\n",
      "Epoch [185/250], Loss: 0.0039\n",
      "Epoch [186/250], Loss: 0.0073\n",
      "Epoch [187/250], Loss: 0.0153\n",
      "Epoch [188/250], Loss: 0.0431\n",
      "Epoch [189/250], Loss: 0.0067\n",
      "Epoch [190/250], Loss: 0.0211\n",
      "Epoch [191/250], Loss: 0.0022\n",
      "Epoch [192/250], Loss: 0.0021\n",
      "Epoch [193/250], Loss: 0.0017\n",
      "Epoch [194/250], Loss: 0.0012\n",
      "Epoch [195/250], Loss: 0.0011\n",
      "Epoch [196/250], Loss: 0.3954\n",
      "Epoch [197/250], Loss: 0.0269\n",
      "Epoch [198/250], Loss: 0.0067\n",
      "Epoch [199/250], Loss: 0.0109\n",
      "Epoch [200/250], Loss: 0.0033\n",
      "Epoch [201/250], Loss: 0.0029\n",
      "Epoch [202/250], Loss: 0.0688\n",
      "Epoch [203/250], Loss: 0.2558\n",
      "Epoch [204/250], Loss: 0.1713\n",
      "Epoch [205/250], Loss: 0.0409\n",
      "Epoch [206/250], Loss: 0.0132\n",
      "Epoch [207/250], Loss: 0.0194\n",
      "Epoch [208/250], Loss: 0.0018\n",
      "Epoch [209/250], Loss: 0.0432\n",
      "Epoch [210/250], Loss: 0.0927\n",
      "Epoch [211/250], Loss: 0.0074\n",
      "Epoch [212/250], Loss: 0.0255\n",
      "Epoch [213/250], Loss: 0.0037\n",
      "Epoch [214/250], Loss: 0.0113\n",
      "Epoch [215/250], Loss: 0.0058\n",
      "Epoch [216/250], Loss: 0.0136\n",
      "Epoch [217/250], Loss: 0.1204\n",
      "Epoch [218/250], Loss: 0.0415\n",
      "Epoch [219/250], Loss: 0.1129\n",
      "Epoch [220/250], Loss: 0.0105\n",
      "Epoch [221/250], Loss: 0.0090\n",
      "Epoch [222/250], Loss: 0.0104\n",
      "Epoch [223/250], Loss: 0.0013\n",
      "Epoch [224/250], Loss: 0.0469\n",
      "Epoch [225/250], Loss: 0.0299\n",
      "Epoch [226/250], Loss: 0.0217\n",
      "Epoch [227/250], Loss: 0.0467\n",
      "Epoch [228/250], Loss: 0.0168\n",
      "Epoch [229/250], Loss: 0.0108\n",
      "Epoch [230/250], Loss: 0.0121\n",
      "Epoch [231/250], Loss: 0.0174\n",
      "Epoch [232/250], Loss: 0.0509\n",
      "Epoch [233/250], Loss: 0.0354\n",
      "Epoch [234/250], Loss: 0.0032\n",
      "Epoch [235/250], Loss: 0.0043\n",
      "Epoch [236/250], Loss: 0.0027\n",
      "Epoch [237/250], Loss: 0.0011\n",
      "Epoch [238/250], Loss: 0.0104\n",
      "Epoch [239/250], Loss: 0.0072\n",
      "Epoch [240/250], Loss: 0.0464\n",
      "Epoch [241/250], Loss: 0.1491\n",
      "Epoch [242/250], Loss: 0.0254\n",
      "Epoch [243/250], Loss: 0.0035\n",
      "Epoch [244/250], Loss: 0.0037\n",
      "Epoch [245/250], Loss: 0.0029\n",
      "Epoch [246/250], Loss: 0.0238\n",
      "Epoch [247/250], Loss: 0.0220\n",
      "Epoch [248/250], Loss: 0.0101\n",
      "Epoch [249/250], Loss: 0.0107\n",
      "Epoch [250/250], Loss: 0.0125\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 250\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(sequences), batch_size):\n",
    "        batch_seq = sequences[i:i + batch_size]\n",
    "        batch_target = targets[i:i + batch_size]\n",
    "        \n",
    "        # Forward Pass\n",
    "        outputs = model(batch_seq)\n",
    "        loss = criterion(outputs, batch_target)\n",
    "        \n",
    "        # Backward Pass and Optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "045604e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T19:14:09.245967Z",
     "iopub.status.busy": "2025-02-11T19:14:09.245652Z",
     "iopub.status.idle": "2025-02-11T19:14:09.337307Z",
     "shell.execute_reply": "2025-02-11T19:14:09.336339Z"
    },
    "papermill": {
     "duration": 0.106463,
     "end_time": "2025-02-11T19:14:09.338836",
     "exception": false,
     "start_time": "2025-02-11T19:14:09.232373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I wandered lonely as a man, Stuff'd with the stuff that is coarse and stuff'd with the stuff that is fine, One of the Nation of many nations, the smallest the same and the largest the same, A Southerner soon as a Northerner, a planter nonchalant and hospitable down by the Oconee I live, A\n"
     ]
    }
   ],
   "source": [
    "def generate_poem(model, seed_text, num_words = 50):\n",
    "    model.eval()\n",
    "    words = seed_text.split()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_words):\n",
    "            # Get the Last `seq_length` Words\n",
    "            seq = [word_to_idx.get(word, 0) for word in words[-seq_length:]]  # Use 0 for OOV Words\n",
    "            seq = torch.tensor(seq, dtype = torch.long).unsqueeze(0)\n",
    "            output = model(seq)\n",
    "\n",
    "            # Apply Softmax\n",
    "            probabilities = F.softmax(output, dim = 1)\n",
    "\n",
    "            # Sample from the Probability Distribution\n",
    "            predicted_idx = torch.multinomial(probabilities, 1).item()\n",
    "\n",
    "            words.append(idx_to_word[predicted_idx])\n",
    "            \n",
    "    return \" \".join(words)\n",
    "\n",
    "# Generate a Poem\n",
    "seed_text = \"I wandered lonely as a\"\n",
    "generated_poem = generate_poem(model, seed_text, num_words = 50)\n",
    "print(generated_poem)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6623322,
     "sourceId": 10689716,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2811.586507,
   "end_time": "2025-02-11T19:14:10.575874",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-11T18:27:18.989367",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
