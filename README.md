# Deep-Learning-Lab

**Lab 1: Experiment 1:** In this experiment, we implemented a fully connected neural network for classifying digits from the MNIST dataset using NumPy. It demonstrates data preprocessing, model training with forward and backward propagation, and evaluation using accuracy and loss metrics. We also used data augmentation methods like random rotation and horizontal flipping to improve generalization.

**Lab 2: Experiment 2:** In this experiment, we trained neural networks on linearly separable and non-linearly separable datasets (E.g., Moon, Circle) using NumPy. It demonstrates the limitations of a network without hidden layers and the improvement with hidden layers and activation functions like ReLU or Sigmoid.

**Lab 3: Experiment 3:** In this experiment, we implemented Convolutional Neural Networks (CNNs) for classifying images from the Cats vs. Dogs and CIFAR-10 datasets. We experimented with different activation functions (ReLU, Tanh, Leaky ReLU), weight initialization techniques (Xavier, Kaiming, Random), and optimizers (SGD, Adam, RMSprop). We trained and evaluated the models using accuracy and loss metrics, saved the best models, and compared them with a fine-tuned ResNet-18 for performance.
