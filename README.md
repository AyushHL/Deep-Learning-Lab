# Deep Learning Lab

**Lab 1: Experiment 1:** In this experiment, we implemented a fully connected neural network for classifying digits from the MNIST dataset using NumPy. It demonstrates data preprocessing, model training with forward and backward propagation, and evaluation using accuracy and loss metrics. We also used data augmentation methods like random rotation and horizontal flipping to improve generalization.

**Lab 2: Experiment 2:** In this experiment, we trained neural networks on linearly separable and non-linearly separable datasets (E.g., Moon, Circle) using NumPy. It demonstrates the limitations of a network without hidden layers and the improvement with hidden layers and activation functions like ReLU or Sigmoid.

**Lab 3: Experiment 3:** In this experiment, we implemented Convolutional Neural Networks (CNNs) for classifying images from the Cats vs. Dogs and CIFAR-10 datasets. We experimented with different activation functions (ReLU, Tanh, Leaky ReLU), weight initialization techniques (Xavier, Kaiming, Random), and optimizers (SGD, Adam, RMSprop). We trained and evaluated the models using accuracy and loss metrics, saved the best models, and compared them with a fine-tuned ResNet-18 for performance.

**Lab 4: Experiment 4:**  In this experiment, we implemented Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs) to generate poetry from a dataset of 100 poems using PyTorch. The experiment involved data preprocessing, tokenization, and vocabulary creation. We explored two approaches: One-Hot Encoding and Trainable Word Embeddings. Both models were trained to predict the next word in a sequence, and we evaluated the models based on their ability to generate coherent text.

**Lab 5: Experiment 5:** In this experiment, we implemented a Sequence-to-Sequence (Seq2Seq) model using LSTMs for English-to-Spanish translation. The experiment includes two architectures: a vanilla encoder-decoder and an attention-based model using Bahdanau and Luong mechanisms. We handled data preprocessing, tokenization, and vocabulary mapping with special tokens. The models were trained with teacher forcing and evaluated using BLEU scores and loss. The attention-based models produced more fluent translations and better context handling.
