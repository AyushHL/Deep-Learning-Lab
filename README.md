# Deep Learning Lab

**Lab 1: Experiment 1:** In this experiment, we implemented a fully connected neural network for classifying digits from the MNIST dataset using NumPy. It demonstrates data preprocessing, model training with forward and backward propagation, and evaluation using accuracy and loss metrics. We also used data augmentation methods like random rotation and horizontal flipping to improve generalization.

**Lab 2: Experiment 2:** In this experiment, we trained neural networks on linearly separable and non-linearly separable datasets (E.g., Moon, Circle) using NumPy. It demonstrates the limitations of a network without hidden layers and the improvement with hidden layers and activation functions like ReLU or Sigmoid.

**Lab 3: Experiment 3:** In this experiment, we implemented Convolutional Neural Networks (CNNs) for classifying images from the Cats vs. Dogs and CIFAR-10 datasets. We experimented with different activation functions (ReLU, Tanh, Leaky ReLU), weight initialization techniques (Xavier, Kaiming, Random), and optimizers (SGD, Adam, RMSprop). We trained and evaluated the models using accuracy and loss metrics, saved the best models, and compared them with a fine-tuned ResNet-18 for performance.

**Lab 4: Experiment 4:** In this experiment, we implemented a Recurrent Neural Network (RNN) to predict and generate poetry using a dataset of 100 poems. It demonstrates data preprocessing, sequence modeling, and text generation using PyTorch. This model utilizes embedding layers, an RNN layer, and a fully connected layer for word prediction. The generated poems showed improved coherence with extended training.
